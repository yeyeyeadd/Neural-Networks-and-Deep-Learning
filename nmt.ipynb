{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjPTaRB4mpCd"
      },
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignment by selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9IS9B9-yUU5"
      },
      "source": [
        "# Setup PyTorch\n",
        "\n",
        "All files will be stored at /content/csc421/a3/ folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-6MQhMOlHXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d009a874-f069-4a58-95c4-2ae22a411ba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "/content/content/csc421/a3/content/csc421/a3\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install Pillow\n",
        "%mkdir -p ./content/csc421/a3/\n",
        "%cd ./content/csc421/a3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DaTdRNuUra7"
      },
      "source": [
        "# Helper code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BIpGwANoQOg"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-UJHBYZkh7f"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "from pathlib import Path\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "\n",
        "def get_file(\n",
        "    fname, origin, untar=False, extract=False, archive_format=\"auto\", cache_dir=\"data\"\n",
        "):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + \".tar.gz\"\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "\n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print(\"Downloading data from\", origin)\n",
        "\n",
        "        error_msg = \"URL fetch failure on {}: {} -- {}\"\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print(\"Extracting file.\")\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "\n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "    Arguments:\n",
        "        tensor: A Tensor object.\n",
        "        cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "    Returns:\n",
        "        A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title(\"BS={}, nhid={}\".format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel(\"Epochs\", fontsize=16)\n",
        "    plt.ylabel(\"Loss\", fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, \"loss_plot.pdf\"))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_loss_comparison_gru(l1, l2, o1, o2, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and val loss curves from GRU runs.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    ax[0].plot(range(len(mean_l1)), mean_l1, label=\"ds=\" + o1.data_file_name)\n",
        "    ax[0].plot(range(len(mean_l2)), mean_l2, label=\"ds=\" + o2.data_file_name)\n",
        "    ax[0].title.set_text(\"Train Loss | GRU Hidden Size = {}\".format(o2.hidden_size))\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[1].plot(range(len(l1[1])), l1[1], label=\"ds=\" + o1.data_file_name)\n",
        "    ax[1].plot(range(len(l2[1])), l2[1], label=\"ds=\" + o2.data_file_name)\n",
        "    ax[1].title.set_text(\"Val Loss | GRU Hidden Size = {}\".format(o2.hidden_size))\n",
        "\n",
        "    ax[0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "    ax[0].set_ylabel(\"Loss\", fontsize=10)\n",
        "    ax[1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "    ax[1].set_ylabel(\"Loss\", fontsize=10)\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"GRU Performance by Dataset\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.85)\n",
        "    plt.legend()\n",
        "\n",
        "    plt_path = \"./loss_plot_{}.pdf\".format(fn)\n",
        "    plt.savefig(plt_path)\n",
        "    print(f\"Plot saved to: {Path(plt_path).resolve()}\")\n",
        "\n",
        "\n",
        "def save_loss_comparison_by_dataset(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
        "    runs in Part 3, comparing by dataset while holding hidden size constant.\n",
        "\n",
        "    Models within each pair (l1, l2) and (l3, l4) have the same hidden sizes.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        l3: Tuple of lists containing training / val losses for model 3.\n",
        "        l4: Tuple of lists containing training / val losses for model 4.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        o3: Options for model 3.\n",
        "        o4: Options for model 4.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n",
        "    mean_l3 = [np.mean(l3[0][i * s : (i + 1) * s]) for i in range(len(l3[0]) // s)]\n",
        "    mean_l4 = [np.mean(l4[0][i * s : (i + 1) * s]) for i in range(len(l4[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label=\"ds=\" + o1.data_file_name)\n",
        "    ax[0][0].plot(range(len(mean_l2)), mean_l2, label=\"ds=\" + o2.data_file_name)\n",
        "    ax[0][0].title.set_text(\n",
        "        \"Train Loss | Model Hidden Size = {}\".format(o1.hidden_size)\n",
        "    )\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[0][1].plot(range(len(l1[1])), l1[1], label=\"ds=\" + o1.data_file_name)\n",
        "    ax[0][1].plot(range(len(l2[1])), l2[1], label=\"ds=\" + o2.data_file_name)\n",
        "    ax[0][1].title.set_text(\"Val Loss | Model Hidden Size = {}\".format(o1.hidden_size))\n",
        "\n",
        "    ax[1][0].plot(range(len(mean_l3)), mean_l3, label=\"ds=\" + o3.data_file_name)\n",
        "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label=\"ds=\" + o4.data_file_name)\n",
        "    ax[1][0].title.set_text(\n",
        "        \"Train Loss | Model Hidden Size = {}\".format(o3.hidden_size)\n",
        "    )\n",
        "\n",
        "    ax[1][1].plot(range(len(l3[1])), l3[1], label=\"ds=\" + o3.data_file_name)\n",
        "    ax[1][1].plot(range(len(l4[1])), l4[1], label=\"ds=\" + o4.data_file_name)\n",
        "    ax[1][1].title.set_text(\"Val Loss | Model Hidden Size = {}\".format(o4.hidden_size))\n",
        "\n",
        "    for i in range(2):\n",
        "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][0].legend(loc=\"upper right\")\n",
        "        ax[i][1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"Performance by Dataset Size\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.9)\n",
        "    plt.legend()\n",
        "    plt.savefig(\"./loss_plot_{}.pdf\".format(fn))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_loss_comparison_by_hidden(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
        "    runs in Part 3, comparing by hidden size while holding dataset constant.\n",
        "\n",
        "    Models within each pair (l1, l3) and (l2, l4) have the same dataset.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        l3: Tuple of lists containing training / val losses for model 3.\n",
        "        l4: Tuple of lists containing training / val losses for model 4.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        o3: Options for model 3.\n",
        "        o4: Options for model 4.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n",
        "    mean_l3 = [np.mean(l3[0][i * s : (i + 1) * s]) for i in range(len(l3[0]) // s)]\n",
        "    mean_l4 = [np.mean(l4[0][i * s : (i + 1) * s]) for i in range(len(l4[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label=\"hid_size=\" + str(o1.hidden_size))\n",
        "    ax[0][0].plot(range(len(mean_l3)), mean_l3, label=\"hid_size=\" + str(o3.hidden_size))\n",
        "    ax[0][0].title.set_text(\"Train Loss | Dataset = \" + o1.data_file_name)\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[0][1].plot(range(len(l1[1])), l1[1], label=\"hid_size=\" + str(o1.hidden_size))\n",
        "    ax[0][1].plot(range(len(l3[1])), l3[1], label=\"hid_size=\" + str(o3.hidden_size))\n",
        "    ax[0][1].title.set_text(\"Val Loss | Dataset = \" + o1.data_file_name)\n",
        "\n",
        "    ax[1][0].plot(range(len(mean_l2)), mean_l2, label=\"hid_size=\" + str(o2.hidden_size))\n",
        "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label=\"hid_size=\" + str(o4.hidden_size))\n",
        "    ax[1][0].title.set_text(\"Train Loss | Dataset = \" + o3.data_file_name)\n",
        "\n",
        "    ax[1][1].plot(range(len(l2[1])), l2[1], label=\"hid_size=\" + str(o2.hidden_size))\n",
        "    ax[1][1].plot(range(len(l4[1])), l4[1], label=\"hid_size=\" + str(o4.hidden_size))\n",
        "    ax[1][1].title.set_text(\"Val Loss | Dataset = \" + o4.data_file_name)\n",
        "\n",
        "    for i in range(2):\n",
        "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][0].legend(loc=\"upper right\")\n",
        "        ax[i][1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"Performance by Hidden State Size\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.9)\n",
        "    plt.legend()\n",
        "    plt.savefig(\"./loss_plot_{}.pdf\".format(fn))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, \"encoder.pt\"), \"wb\") as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, \"decoder.pt\"), \"wb\") as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, \"idx_dict.pkl\"), \"wb\") as f:\n",
        "        pkl.dump(idx_dict, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbvpn4MaV0I1"
      },
      "source": [
        "## Data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVT4TNTOV3Eg"
      },
      "outputs": [],
      "source": [
        "def read_lines(filename):\n",
        "    \"\"\"Read a file and split it into lines.\"\"\"\n",
        "    lines = open(filename).read().strip().lower().split(\"\\n\")\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_pairs(filename):\n",
        "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
        "\n",
        "    Returns:\n",
        "        source_words: A list of the first word in each line of the file.\n",
        "        target_words: A list of the second word in each line of the file.\n",
        "    \"\"\"\n",
        "    lines = read_lines(filename)\n",
        "    source_words, target_words = [], []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            source, target = line.split()\n",
        "            source_words.append(source)\n",
        "            target_words.append(target)\n",
        "    return source_words, target_words\n",
        "\n",
        "\n",
        "def all_alpha_or_dash(s):\n",
        "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\"\"\"\n",
        "    return all(c.isalpha() or c == \"-\" for c in s)\n",
        "\n",
        "\n",
        "def filter_lines(lines):\n",
        "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\"\"\"\n",
        "    return [line for line in lines if all_alpha_or_dash(line)]\n",
        "\n",
        "\n",
        "def load_data(file_name):\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\"\"\"\n",
        "    path = \"./data/{}.txt\".format(file_name)\n",
        "    source_lines, target_lines = read_pairs(path)\n",
        "\n",
        "    # Filter lines\n",
        "    source_lines = filter_lines(source_lines)\n",
        "    target_lines = filter_lines(target_lines)\n",
        "\n",
        "    all_characters = set(\"\".join(source_lines)) | set(\"\".join(target_lines))\n",
        "\n",
        "    # Create a dictionary mapping each character to a unique index\n",
        "    char_to_index = {\n",
        "        char: index for (index, char) in enumerate(sorted(list(all_characters)))\n",
        "    }\n",
        "\n",
        "    # Add start and end tokens to the dictionary\n",
        "    start_token = len(char_to_index)\n",
        "    end_token = len(char_to_index) + 1\n",
        "    char_to_index[\"SOS\"] = start_token\n",
        "    char_to_index[\"EOS\"] = end_token\n",
        "\n",
        "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
        "    index_to_char = {index: char for (char, index) in char_to_index.items()}\n",
        "\n",
        "    # Store the final size of the vocabulary\n",
        "    vocab_size = len(char_to_index)\n",
        "\n",
        "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
        "\n",
        "    idx_dict = {\n",
        "        \"char_to_index\": char_to_index,\n",
        "        \"index_to_char\": index_to_char,\n",
        "        \"start_token\": start_token,\n",
        "        \"end_token\": end_token,\n",
        "    }\n",
        "\n",
        "    return line_pairs, vocab_size, idx_dict\n",
        "\n",
        "\n",
        "def create_dict(pairs):\n",
        "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
        "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
        "    all source indexes and the other containing all corresponding target indexes.\n",
        "    Within a batch, all the source words are the same length, and all the target words are\n",
        "    the same length.\n",
        "    \"\"\"\n",
        "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for (s, t) in unique_pairs:\n",
        "        d[(len(s), len(t))].append((s, t))\n",
        "\n",
        "    return d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRWfRdmVVjUl"
      },
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wa5-onJhoSeM"
      },
      "outputs": [],
      "source": [
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\"\"\"\n",
        "    return [char_to_index[char] for char in s] + [\n",
        "        end_token\n",
        "    ]  # Adds the end token to each index list\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "        line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
        "    return \" \".join(\n",
        "        [translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()]\n",
        "    )\n",
        "\n",
        "\n",
        "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\"\"\"\n",
        "\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "    index_to_char = idx_dict[\"index_to_char\"]\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = \"\"\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(\n",
        "        torch.LongTensor(indexes).unsqueeze(0), opts.cuda\n",
        "    )  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_last_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_last_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(\n",
        "            decoder_inputs, encoder_annotations, decoder_hidden\n",
        "        )\n",
        "\n",
        "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1]  # latest output token\n",
        "\n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [\n",
        "                    index_to_char[int(item)]\n",
        "                    for item in generated_words.cpu().numpy().reshape(-1)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\"\"\"\n",
        "    if idx_dict is None:\n",
        "        line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "    index_to_char = idx_dict[\"index_to_char\"]\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = \"\"\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(\n",
        "        torch.LongTensor(indexes).unsqueeze(0), opts.cuda\n",
        "    )  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(\n",
        "            decoder_inputs, encoder_annotations, decoder_hidden\n",
        "        )\n",
        "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1]  # latest output token\n",
        "\n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [\n",
        "                    index_to_char[int(item)]\n",
        "                    for item in generated_words.cpu().numpy().reshape(-1)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "    if isinstance(attention_weights, tuple):\n",
        "        ## transformer's attention mweights\n",
        "        attention_weights, self_attention_weights = attention_weights\n",
        "\n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "\n",
        "    for i in range(len(all_attention_weights)):\n",
        "        attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)\n",
        "        cax = ax.matshow(attention_weights_matrix, cmap=\"bone\")\n",
        "        fig.colorbar(cax)\n",
        "\n",
        "        # Set up axes\n",
        "        ax.set_yticklabels([\"\"] + list(input_string) + [\"EOS\"], rotation=90)\n",
        "        ax.set_xticklabels(\n",
        "            [\"\"] + list(gen_string) + ([\"EOS\"] if produced_end_token else [])\n",
        "        )\n",
        "\n",
        "        # Show label at every tick\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        # Add title\n",
        "        plt.xlabel(\"Attention weights to the source sentence in layer {}\".format(i + 1))\n",
        "        plt.tight_layout()\n",
        "        plt.grid(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model.\n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [\n",
        "            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n",
        "            for s in input_strings\n",
        "        ]\n",
        "        target_tensors = [\n",
        "            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n",
        "            for s in target_strings\n",
        "        ]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            start_vector = (\n",
        "                torch.ones(BS).long().unsqueeze(1) * start_token\n",
        "            )  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat(\n",
        "                [decoder_input, targets[:, 0:-1]], dim=1\n",
        "            )  # Gets decoder inputs by shifting the targets to the right\n",
        "\n",
        "            decoder_outputs, attention_weights = decoder(\n",
        "                decoder_inputs, encoder_annotations, decoder_hidden\n",
        "            )\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "\n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "                # Zero gradients\n",
        "                optimizer.zero_grad()\n",
        "                # Compute gradients\n",
        "                loss.backward()\n",
        "                # Update the parameters of the encoder and decoder\n",
        "                optimizer.step()\n",
        "\n",
        "    return losses\n",
        "\n",
        "\n",
        "def training_loop(\n",
        "    train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts\n",
        "):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "        * Returns loss curves for comparison\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        losses: Lists containing training and validation loss curves.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, \"loss_log.txt\"), \"w\")\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    mean_train_losses = []\n",
        "    mean_val_losses = []\n",
        "\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0][\"lr\"] *= opts.lr_decay\n",
        "\n",
        "        train_loss = compute_loss(\n",
        "            train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts\n",
        "        )\n",
        "        val_loss = compute_loss(\n",
        "            val_dict, encoder, decoder, idx_dict, criterion, None, opts\n",
        "        )\n",
        "\n",
        "        mean_train_loss = np.mean(train_loss)\n",
        "        mean_val_loss = np.mean(val_loss)\n",
        "\n",
        "        if mean_val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "            best_val_loss = mean_val_loss\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        if early_stopping_counter > opts.early_stopping_patience:\n",
        "            print(\n",
        "                \"Validation loss has not improved in {} epochs, stopping early\".format(\n",
        "                    opts.early_stopping_patience\n",
        "                )\n",
        "            )\n",
        "            print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "            return (train_losses, mean_val_losses)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
        "        print(\n",
        "            \"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(\n",
        "                epoch, mean_train_loss, mean_val_loss, gen_string\n",
        "            )\n",
        "        )\n",
        "\n",
        "        loss_log.write(\"{} {} {}\\n\".format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses += train_loss\n",
        "        val_losses += val_loss\n",
        "\n",
        "        mean_train_losses.append(mean_train_loss)\n",
        "        mean_val_losses.append(mean_val_loss)\n",
        "\n",
        "        save_loss_plot(mean_train_losses, mean_val_losses, opts)\n",
        "\n",
        "    print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "    return (train_losses, mean_val_losses)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Data Stats\".center(80))\n",
        "    print(\"-\" * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print(\"Num unique word pairs: {}\".format(len(line_pairs)))\n",
        "    print(\"Vocabulary: {}\".format(idx_dict[\"char_to_index\"].keys()))\n",
        "    print(\"Vocab size: {}\".format(vocab_size))\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    if opts.encoder_type == \"rnn\":\n",
        "        encoder = GRUEncoder(\n",
        "            vocab_size=vocab_size, hidden_size=opts.hidden_size, opts=opts\n",
        "        )\n",
        "    elif opts.encoder_type == \"transformer\":\n",
        "        encoder = TransformerEncoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "            num_layers=opts.num_transformer_layers,\n",
        "            opts=opts,\n",
        "        )\n",
        "    elif opts.encoder_type == \"attention\":\n",
        "      encoder = AttentionEncoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "            opts=opts,\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if opts.decoder_type == \"rnn\":\n",
        "        decoder = RNNDecoder(vocab_size=vocab_size, hidden_size=opts.hidden_size)\n",
        "    elif opts.decoder_type == \"rnn_attention\":\n",
        "        decoder = RNNAttentionDecoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "            attention_type=opts.attention_type,\n",
        "        )\n",
        "    elif opts.decoder_type == \"transformer\":\n",
        "        decoder = TransformerDecoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "            num_layers=opts.num_transformer_layers,\n",
        "        )\n",
        "    elif opts.encoder_type == \"attention\":\n",
        "      decoder = AttentionDecoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    #### setup checkpoint path\n",
        "    model_name = \"h{}-bs{}-{}-{}\".format(\n",
        "        opts.hidden_size, opts.batch_size, opts.decoder_type, opts.data_file_name\n",
        "    )\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(\n",
        "        list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        losses = training_loop(\n",
        "            train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts\n",
        "        )\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Exiting early from training.\")\n",
        "        return encoder, decoder, losses\n",
        "\n",
        "    return encoder, decoder, losses\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Opts\".center(80))\n",
        "    print(\"-\" * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print(\"{:>30}: {:<30}\".format(key, opts.__dict__[key]).center(80))\n",
        "    print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yh08KhgnA30"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aROU2xZanDKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97931a1d-88cc-4ee7-a902-becc786e6975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/pig_latin_small.txt\n",
            "Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_small.txt\n",
            "data/pig_latin_large.txt\n",
            "Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_large.txt\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(\n",
        "    fname=\"pig_latin_small.txt\",\n",
        "    origin=\"http://www.cs.toronto.edu/~jba/pig_latin_small.txt\",\n",
        "    untar=False,\n",
        ")\n",
        "\n",
        "data_fpath = get_file(\n",
        "    fname=\"pig_latin_large.txt\",\n",
        "    origin=\"http://www.cs.toronto.edu/~jba/pig_latin_large.txt\",\n",
        "    untar=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDYMr7NclZdw"
      },
      "source": [
        "# Part 1: Neural machine translation (NMT)\n",
        "\n",
        "In this section, you will implement a Gated Recurrent Unit (GRU) cell, a common type of recurrent neural network (RNN). The GRU cell is a simplification of the Long Short-Term Memory cell. Therefore, we have provided you with an implemented LSTM cell (`MyLSTMCell`), which you can reference when completing `MyGRUCell`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOnALRQkkjDO"
      },
      "outputs": [],
      "source": [
        "class MyLSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyLSTMCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Wif = nn.Linear(input_size, hidden_size)\n",
        "        self.Whf = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wii = nn.Linear(input_size, hidden_size)\n",
        "        self.Whi = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wic = nn.Linear(input_size, hidden_size)\n",
        "        self.Whc = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wio = nn.Linear(input_size, hidden_size)\n",
        "        self.Who = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, x, h_prev, c_prev):\n",
        "        \"\"\"Forward pass of the LSTM computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "            c_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "            c_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "\n",
        "        f = torch.sigmoid(self.Wif(x) + self.Whf(h_prev))\n",
        "        i = torch.sigmoid(self.Wii(x) + self.Whi(h_prev))\n",
        "\n",
        "        c = torch.tanh(self.Wic(x) + self.Whc(h_prev))\n",
        "        o = torch.sigmoid(self.Wio(x) + self.Who(h_prev))\n",
        "\n",
        "        c_new = f * c_prev + i * c\n",
        "        h_new = o * torch.tanh(c_new)\n",
        "\n",
        "        return h_new, c_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCae1mOUlZrC"
      },
      "source": [
        "## Step 1: GRU Cell\n",
        "Please implement the `MyGRUCell` class defined in the next cell. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGyxqZIQzTJH"
      },
      "outputs": [],
      "source": [
        "class MyGRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyGRUCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        # Input linear layers\n",
        "        self.Wiz = nn.Linear(input_size, hidden_size)\n",
        "        self.Wir = nn.Linear(input_size, hidden_size)\n",
        "        self.Wih = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        # Hidden linear layers\n",
        "        self.Whz =  nn.Linear(hidden_size, hidden_size)\n",
        "        self.Whr =  nn.Linear(hidden_size, hidden_size)\n",
        "        self.Whh =  nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"Forward pass of the GRU computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        z = torch.sigmoid(self.Wiz(x) + self.Whz(h_prev))\n",
        "        r = torch.sigmoid(self.Wir(x) + self.Whr(h_prev))\n",
        "        g = torch.tanh(self.Wih(x) + self.Whh(r * h_prev))\n",
        "        h_new = (1-z)*h_prev + z*g\n",
        "        return h_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecEq4TP2lZ4Z"
      },
      "source": [
        "## Step 2: GRU Encoder\n",
        "\n",
        "The following cells use your `MyGRUCell` implementation to build a recurrent encoder and decoder. Please read the implementations to understand what they do and run the cells before proceeding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jDNim2fmVJV"
      },
      "outputs": [],
      "source": [
        "class GRUEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(GRUEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = MyGRUCell(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "        annotations = []\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = encoded[:, i, :]  # Get the current time step, across the whole batch\n",
        "            hidden = self.gru(x, hidden)\n",
        "            annotations.append(hidden)\n",
        "\n",
        "        annotations = torch.stack(annotations, dim=1)\n",
        "        return annotations, hidden\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
        "        of a batch of sequences.\n",
        "\n",
        "        Arguments:\n",
        "            bs: The batch size for the initial hidden state.\n",
        "\n",
        "        Returns:\n",
        "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvwizYM9ma4p"
      },
      "outputs": [],
      "source": [
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
        "            annotations: This is not used here. It just maintains consistency with the\n",
        "                    interface used by the AttentionDecoder class.\n",
        "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            None\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        hiddens = []\n",
        "        h_prev = hidden_init\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = embed[\n",
        "                :, i, :\n",
        "            ]  # Get the current time step input tokens, across the whole batch\n",
        "            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n",
        "            hiddens.append(h_prev)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSDTbsydlaGI"
      },
      "source": [
        "## Step 3: Training and Analysis\n",
        "\n",
        "Train the encoder-decoder model to perform English --> Pig Latin translation. We will start by training on the smaller dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmVuXTozTPF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f500e08-9026-448d-85fd-11f69c676c87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 20                                     \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('insult', 'insultway')\n",
            "('march', 'archmay')\n",
            "('to', 'otay')\n",
            "('prove', 'ovepray')\n",
            "('soft', 'oftsay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.335 | Val loss: 2.027 | Gen: ingay-ingay-ay-ingay ay-ingay-ay-ingay-ay inssay-ingay-ingay-a ingay-ingay-ay-ingay inssay-ingay-ingay-a\n",
            "Epoch:   1 | Train loss: 1.882 | Val loss: 1.857 | Gen: eday-onday-onday-ond ay-ingay-onseday-ons onseday-onseday-onsa ingway-onsay-onsay-o ongsay-onseday-onsay\n",
            "Epoch:   2 | Train loss: 1.717 | Val loss: 1.747 | Gen: ededay-oday ay-ingway ongsay-onday-onday-o ingway ongsay-oday-oday\n",
            "Epoch:   3 | Train loss: 1.588 | Val loss: 1.668 | Gen: eday-oday antingway ongingway ingway ongsay-oday\n",
            "Epoch:   4 | Train loss: 1.476 | Val loss: 1.621 | Gen: eday ay-ingway ongsay-illway-illway issway odgay-odgay\n",
            "Epoch:   5 | Train loss: 1.387 | Val loss: 1.540 | Gen: eday ay-ay-oay ongingsway-ingway issway odgay-illway\n",
            "Epoch:   6 | Train loss: 1.299 | Val loss: 1.561 | Gen: edgay ay-ay-ay-oationsway ongingsay-illway-ill issay otationsway\n",
            "Epoch:   7 | Train loss: 1.254 | Val loss: 1.540 | Gen: edway ay ongingsay-ondingway issway odgay-illway\n",
            "Epoch:   8 | Train loss: 1.224 | Val loss: 1.489 | Gen: eday ay ongsay iway ortingway\n",
            "Epoch:   9 | Train loss: 1.148 | Val loss: 1.422 | Gen: eday aishay ondingsay-oringway iway oringway\n",
            "Epoch:  10 | Train loss: 1.088 | Val loss: 1.438 | Gen: edday aishay ondingway-ingway iway ortingway\n",
            "Epoch:  11 | Train loss: 1.050 | Val loss: 1.415 | Gen: eday ay ondingingway issshay ortingway\n",
            "Epoch:  12 | Train loss: 1.013 | Val loss: 1.383 | Gen: edday aillway ondingingway-ingsray issway oringingway\n",
            "Epoch:  13 | Train loss: 0.972 | Val loss: 1.376 | Gen: ehay ay ondingingway-ondersw isssway oringingway\n",
            "Epoch:  14 | Train loss: 0.951 | Val loss: 1.380 | Gen: ehay away ondingingway-ingsray isway ortingway\n",
            "Epoch:  15 | Train loss: 0.938 | Val loss: 1.372 | Gen: edday away ondingingday issshay ortingsray\n",
            "Epoch:  16 | Train loss: 0.923 | Val loss: 1.343 | Gen: ehay ayway ondingingway issshay ortindingway\n",
            "Epoch:  17 | Train loss: 0.885 | Val loss: 1.324 | Gen: ehay aindlay ondingingway-aturedn isway ortingsray-aturangay\n",
            "Epoch:  18 | Train loss: 0.851 | Val loss: 1.325 | Gen: ehay aidway ondingingsay-orhay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.827 | Val loss: 1.304 | Gen: ehay aindlay ondingsay-oringway issay orkingway\n",
            "Epoch:  20 | Train loss: 0.812 | Val loss: 1.285 | Gen: ehay aindlay ondingsay-ingsay issway ortingway\n",
            "Epoch:  21 | Train loss: 0.801 | Val loss: 1.325 | Gen: ehay aindlay ondingingsay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.802 | Val loss: 1.358 | Gen: ehay aidedway ondingingway isway oringay\n",
            "Epoch:  23 | Train loss: 0.772 | Val loss: 1.316 | Gen: ehay ayway ondingingay isway ortingway\n",
            "Epoch:  24 | Train loss: 0.753 | Val loss: 1.334 | Gen: ehay aidedway ondingingway isway ortingway\n",
            "Epoch:  25 | Train loss: 0.740 | Val loss: 1.374 | Gen: ehay ayway ondingsay-oringway issway ortittingway\n",
            "Epoch:  26 | Train loss: 0.738 | Val loss: 1.309 | Gen: ehay ayway ondingingway isway orkingway\n",
            "Epoch:  27 | Train loss: 0.718 | Val loss: 1.298 | Gen: ehay airway ondingingway isway ortingway\n",
            "Epoch:  28 | Train loss: 0.729 | Val loss: 1.290 | Gen: ehay ayway ondingingway isway orkingway\n",
            "Epoch:  29 | Train loss: 0.709 | Val loss: 1.270 | Gen: ehay aindlay ondingingway isway orkingway\n",
            "Epoch:  30 | Train loss: 0.680 | Val loss: 1.260 | Gen: ehay airway ondingingway-oatingw isway orringway\n",
            "Epoch:  31 | Train loss: 0.664 | Val loss: 1.242 | Gen: ehay airway oningingway isway oringingway\n",
            "Epoch:  32 | Train loss: 0.655 | Val loss: 1.324 | Gen: ehay ayway ondingingway isway ortingway\n",
            "Epoch:  33 | Train loss: 0.652 | Val loss: 1.271 | Gen: ehay airway ondingingway isway orkingway\n",
            "Epoch:  34 | Train loss: 0.629 | Val loss: 1.251 | Gen: ehay airway ondingingway isway ortingway\n",
            "Epoch:  35 | Train loss: 0.636 | Val loss: 1.392 | Gen: ehay airway ondictionday-aturing isway orringway\n",
            "Epoch:  36 | Train loss: 0.675 | Val loss: 1.320 | Gen: ehay array ondingingtay issway ortitedway\n",
            "Epoch:  37 | Train loss: 0.654 | Val loss: 1.277 | Gen: ehay airway onitionday isway orkingway\n",
            "Epoch:  38 | Train loss: 0.637 | Val loss: 1.302 | Gen: ehay ayway oningingway-orstay isway omingway\n",
            "Epoch:  39 | Train loss: 0.638 | Val loss: 1.278 | Gen: ehay aindlay onitificationmay isway oringingway\n",
            "Epoch:  40 | Train loss: 0.609 | Val loss: 1.252 | Gen: etray ayway ondingingway isway orkingway\n",
            "Epoch:  41 | Train loss: 0.598 | Val loss: 1.233 | Gen: etay airway ondiciencilicatingwa isway oringingway\n",
            "Epoch:  42 | Train loss: 0.588 | Val loss: 1.311 | Gen: etray ayway onicingway-ingshay isssway ortingway\n",
            "Epoch:  43 | Train loss: 0.607 | Val loss: 1.295 | Gen: ethay airway ontificationsway isway orkingway\n",
            "Epoch:  44 | Train loss: 0.578 | Val loss: 1.255 | Gen: ethay ayway ondiciencilatingway isway orgingway\n",
            "Epoch:  45 | Train loss: 0.557 | Val loss: 1.240 | Gen: ethay ayway ondicienway-ingsray isway orkingway\n",
            "Epoch:  46 | Train loss: 0.545 | Val loss: 1.269 | Gen: ethay ayway ondiciencypay-aintwa isway orkingway\n",
            "Epoch:  47 | Train loss: 0.555 | Val loss: 1.267 | Gen: etay ayway ontificationmay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.548 | Val loss: 1.282 | Gen: ethay ayway ondingingway isway orkingway\n",
            "Epoch:  49 | Train loss: 0.537 | Val loss: 1.241 | Gen: ethay array ondingingway isway orkingway\n",
            "Obtained lowest validation loss of: 1.2331877013911372\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay array ondingingway isway orkingway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "rnn_args_s = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_small\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 50,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"early_stopping_patience\": 20,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 32,\n",
        "    \"encoder_type\": \"rnn\",  # options: rnn / transformer\n",
        "    \"decoder_type\": \"rnn\",  # options: rnn / rnn_attention / transformer\n",
        "    \"attention_type\": \"\",   # options: additive / scaled_dot\n",
        "}\n",
        "rnn_args_s.update(args_dict)\n",
        "\n",
        "print_opts(rnn_args_s)\n",
        "rnn_encode_s, rnn_decoder_s, rnn_losses_s = train(rnn_args_s)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, rnn_encode_s, rnn_decoder_s, None, rnn_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mR97V_NtER6"
      },
      "source": [
        "Next, we train on the larger dataset. This experiment investigates if increasing dataset size improves model generalization on the validation set. \n",
        "\n",
        "For a fair comparison, the number of iterations (not number of epochs) for each run should be similar. This is done in a quick and dirty way by adjusting the batch size so approximately the same number of batches is processed per epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3YLrAjsmx_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "492c2465-82fb-4d7f-c0d8-79ac478a3b48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 10                                     \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('ninja', 'injanay')\n",
            "('iteration', 'iterationway')\n",
            "('chick', 'ickchay')\n",
            "('innovation', 'innovationway')\n",
            "('eclat', 'eclatway')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.285 | Val loss: 2.143 | Gen: ay away onteray ay ontay\n",
            "Epoch:   1 | Train loss: 1.872 | Val loss: 1.988 | Gen: estay away ontay-ay ingshay ontay-ay\n",
            "Epoch:   2 | Train loss: 1.739 | Val loss: 1.881 | Gen: estay-ayday away ontay-ayday ingray ongray-ayday\n",
            "Epoch:   3 | Train loss: 1.619 | Val loss: 1.809 | Gen: estay away ontay-awlay intersay-ayday oughtonway\n",
            "Epoch:   4 | Train loss: 1.524 | Val loss: 1.789 | Gen: eway away onghay-onday-ayday ingsay oughtonway-ayday\n",
            "Epoch:   5 | Train loss: 1.446 | Val loss: 1.731 | Gen: eaway away onsinglay-ayday ingsay oughtionghay-ayday\n",
            "Epoch:   6 | Train loss: 1.365 | Val loss: 1.716 | Gen: eway away-ayday onsay-ingsray isedway oughtinghterghay-oda\n",
            "Epoch:   7 | Train loss: 1.309 | Val loss: 1.663 | Gen: eaway arishedway onsinghtingnay-awlay isedway oughtinghay-oday-ayd\n",
            "Epoch:   8 | Train loss: 1.256 | Val loss: 1.670 | Gen: eway-ayday aisedway onsinghtingnay-awlay isedway oughtingnay-ayday\n",
            "Epoch:   9 | Train loss: 1.222 | Val loss: 1.679 | Gen: etay-awlay ariontay inghtergonday-awlay isedway onghay-oday-ayday\n",
            "Epoch:  10 | Train loss: 1.185 | Val loss: 1.639 | Gen: eway arishedway onsinghtinghtentingn isedway onghay-oday-awlay\n",
            "Epoch:  11 | Train loss: 1.144 | Val loss: 1.673 | Gen: etay-ayday aishedway inghtergroughtergrou isway ongay-ontay-awlay-ay\n",
            "Epoch:  12 | Train loss: 1.110 | Val loss: 1.631 | Gen: eway ayiway onsioncay istay onway-ayday\n",
            "Epoch:  13 | Train loss: 1.068 | Val loss: 1.611 | Gen: eway aristay onsionsay istay ongsay-ayday\n",
            "Epoch:  14 | Train loss: 1.046 | Val loss: 1.623 | Gen: eway-ayday ailway onioday-ayday istay oway-ingsray\n",
            "Epoch:  15 | Train loss: 1.030 | Val loss: 1.596 | Gen: eway aristay onsingsway istay ongsay-ayday\n",
            "Epoch:  16 | Train loss: 0.993 | Val loss: 1.557 | Gen: earoway airay onioday-ayday isway ongingnay\n",
            "Epoch:  17 | Train loss: 0.963 | Val loss: 1.555 | Gen: eateway airay onsiontay-awlay-ayda isway oway-ingsray\n",
            "Epoch:  18 | Train loss: 0.986 | Val loss: 1.732 | Gen: etay-ofway airationsway onsingcay istay ockongormay\n",
            "Epoch:  19 | Train loss: 1.016 | Val loss: 1.542 | Gen: etay airay oningstray-ingsray isway ongionsway\n",
            "Epoch:  20 | Train loss: 0.933 | Val loss: 1.494 | Gen: etay airay onioday-inway-awlay isway onghighay\n",
            "Epoch:  21 | Train loss: 0.894 | Val loss: 1.504 | Gen: etay airay oniodinggray-inway-a isway oway-ightnay\n",
            "Epoch:  22 | Train loss: 0.879 | Val loss: 1.505 | Gen: etay airay oningstray-ingsray isway ongioncay\n",
            "Epoch:  23 | Train loss: 0.863 | Val loss: 1.506 | Gen: etay airay onistionsway istay oway-ightnay\n",
            "Epoch:  24 | Train loss: 0.844 | Val loss: 1.504 | Gen: etay airay oningshay-inway-awla isway owinghay\n",
            "Epoch:  25 | Train loss: 0.834 | Val loss: 1.490 | Gen: etay airardway oningconcay istay onghighay\n",
            "Epoch:  26 | Train loss: 0.829 | Val loss: 1.530 | Gen: etay airay oniodingbray-inway-a isway owilighay\n",
            "Epoch:  27 | Train loss: 0.819 | Val loss: 1.539 | Gen: ethengy airationcay oningstray-ingsray isway ongioncay\n",
            "Epoch:  28 | Train loss: 0.813 | Val loss: 1.495 | Gen: etay airay oniconday-ayday isway oblighay\n",
            "Epoch:  29 | Train loss: 0.793 | Val loss: 1.503 | Gen: etay airay ondibishedway isway onghigbay\n",
            "Epoch:  30 | Train loss: 0.783 | Val loss: 1.503 | Gen: ethay airay onishishedway ishay orgingbay\n",
            "Epoch:  31 | Train loss: 0.763 | Val loss: 1.494 | Gen: ehay airay onionsingsway-ayday isway ongionmay\n",
            "Epoch:  32 | Train loss: 0.753 | Val loss: 1.502 | Gen: ethay airay onichbray-inway-awla isway orkingbay\n",
            "Epoch:  33 | Train loss: 0.732 | Val loss: 1.498 | Gen: ehay airay oniodingstray-inway- isway orgingshay\n",
            "Epoch:  34 | Train loss: 0.712 | Val loss: 1.471 | Gen: ethay airay oniodingcay isway orkingsay\n",
            "Epoch:  35 | Train loss: 0.706 | Val loss: 1.578 | Gen: ethay airaitway oniocousionmernistra issay orkingsbay\n",
            "Epoch:  36 | Train loss: 0.704 | Val loss: 1.712 | Gen: etheway airailway oningiondingstray-in isway orgingbray\n",
            "Epoch:  37 | Train loss: 0.702 | Val loss: 1.493 | Gen: ethay airailway onticondingcay isway orkingbray\n",
            "Epoch:  38 | Train loss: 0.686 | Val loss: 1.523 | Gen: ehetay airaitway onionsingstray-achhe isway orgingsgray\n",
            "Epoch:  39 | Train loss: 0.679 | Val loss: 1.461 | Gen: ehytay airay onionsitingway isway orkingbay\n",
            "Epoch:  40 | Train loss: 0.673 | Val loss: 1.577 | Gen: etheway airaway oningstionsay-inway- isway orkinghay\n",
            "Epoch:  41 | Train loss: 0.677 | Val loss: 1.477 | Gen: ehay airay oningionsay-inway-aw isway orkingway\n",
            "Epoch:  42 | Train loss: 0.644 | Val loss: 1.457 | Gen: ehytay airay onintionsay-inway-aw isway orkingsay\n",
            "Epoch:  43 | Train loss: 0.638 | Val loss: 1.471 | Gen: ehay airay onionsitingntay isway orkingbay\n",
            "Epoch:  44 | Train loss: 0.628 | Val loss: 1.479 | Gen: ehytay airatway ondingscay-inway-awa isway orksay-inway-awlay\n",
            "Epoch:  45 | Train loss: 0.620 | Val loss: 1.546 | Gen: ehay airiationay oningionsingcay isway orkingsay\n",
            "Epoch:  46 | Train loss: 0.620 | Val loss: 1.562 | Gen: ehay airailway ondinishiciay-away-a issay orkingbay\n",
            "Epoch:  47 | Train loss: 0.615 | Val loss: 1.498 | Gen: ehay airay ontinishedway isway orkingbay\n",
            "Epoch:  48 | Train loss: 0.603 | Val loss: 1.638 | Gen: etheway airailway ondinishedway isway orginghay-ayday\n",
            "Epoch:  49 | Train loss: 0.617 | Val loss: 1.564 | Gen: ethay airay ondingscay-oushingra isway orkingbay\n",
            "Obtained lowest validation loss of: 1.456797340956445\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airay ondingscay-oushingra isway orkingbay\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "rnn_args_l = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_large\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 50,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"early_stopping_patience\": 10,\n",
        "    \"batch_size\": 512,\n",
        "    \"hidden_size\": 32,\n",
        "    \"encoder_type\": \"rnn\",  # options: rnn / transformer\n",
        "    \"decoder_type\": \"rnn\",  # options: rnn / rnn_attention / transformer\n",
        "    \"attention_type\": \"\",   # options: additive / scaled_dot\n",
        "}\n",
        "rnn_args_l.update(args_dict)\n",
        "\n",
        "print_opts(rnn_args_l)\n",
        "rnn_encode_l, rnn_decoder_l, rnn_losses_l = train(rnn_args_l)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, rnn_encode_l, rnn_decoder_l, None, rnn_args_l\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01HsZ6EItc56"
      },
      "source": [
        "The code below plots the training and validation losses of each model, as a function of the number of gradient descent iterations. Are there significant differences in the validation performance of each model? (see follow-up questions in handout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qyk_9-Fwtekj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "a48ca3cd-06c0-4681-da77-3ec0d1fcb133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plot saved to: /content/content/csc421/a3/content/csc421/a3/loss_plot_gru.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "save_loss_comparison_gru(rnn_losses_s, rnn_losses_l, rnn_args_s, rnn_args_l, \"gru\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE4ijaCzneAt"
      },
      "source": [
        "Select best performing model, and try translating different sentences by changing the variable TEST_SENTENCE. Identify a failure mode and briefly describe it (see follow-up questions in handout)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrNnz8W1nULf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68227657-dba8-4dcf-dbeb-17c280c2cc84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source:\t\ti love to take a shower at shopping mall thanks \n",
            "translated:\tiway ovedway otay aketay amay owededway atay opinghay allway ankshay\n"
          ]
        }
      ],
      "source": [
        "best_encoder = rnn_encode_s  # Replace with rnn_encode_s or rnn_encode_l\n",
        "best_decoder = rnn_decoder_s  # Replace with rnn_decoder_s or rnn_decoder_l\n",
        "best_args = rnn_args_s     # Replace with rnn_args_s or rnn_args_l\n",
        "\n",
        "TEST_SENTENCE = \"i love to take a shower at shopping mall thanks\"\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, best_encoder, best_decoder, None, best_args\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 1: Neural machine translation report: \n",
        "\n",
        "1. The larger dataset model preform significantly better, as shown by lower loss. The reason for that larger dataset may have more information than the smaller dataset, so it will improve our learning.\n",
        "\n",
        "2. After few attempts, I find out the model has poor performance on langer words, and on consonant pairs.\n",
        "\n",
        "3. Total number parameters of the LSTM: \n",
        "\n"
      ],
      "metadata": {
        "id": "hOnffzNUxlB5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWwA6OGqlaTq"
      },
      "source": [
        "# Part 2: Attention mechanisms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJSafHSAmu_w"
      },
      "source": [
        "## Step 1: Additive attention\n",
        "\n",
        "In the next cell, the [additive attention](https://paperswithcode.com/method/additive-attention) mechanism has been implemented for you. Please take a momement to read through it and understand what it is doing. See the assignment handouts for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdewEVSMo5jJ"
      },
      "outputs": [],
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # A two layer fully-connected network\n",
        "        # hidden_size * 2 --> hidden_size, ReLU, hidden_size --> 1\n",
        "        self.attention_network = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1),\n",
        "        )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the additive attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "        batch_size = keys.size(0)\n",
        "        expanded_queries = queries.view(batch_size, -1, self.hidden_size).expand_as(\n",
        "            keys\n",
        "        )\n",
        "        concat_inputs = torch.cat([expanded_queries, keys], dim=2)\n",
        "        unnormalized_attention = self.attention_network(concat_inputs)\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(attention_weights.transpose(2, 1), values)\n",
        "        return context, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73_p8d5EmvOJ"
      },
      "source": [
        "## Step 2: RNN + additive attention\n",
        "\n",
        "In the next cell, a modification of our `RNNDecoder` that makes use of an additive attention mechanism as been implemented for your. Please take a momement to read through it and understand what it is doing. See the assignment handouts for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJaABkXrpJSw"
      },
      "outputs": [],
      "source": [
        "class RNNAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, attention_type=\"scaled_dot\"):\n",
        "        super(RNNAttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size * 2, hidden_size=hidden_size)\n",
        "        if attention_type == \"additive\":\n",
        "            self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
        "        elif attention_type == \"scaled_dot\":\n",
        "            self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
        "\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        hiddens = []\n",
        "        attentions = []\n",
        "        h_prev = hidden_init\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            embed_current = embed[\n",
        "                :, i, :\n",
        "            ]  # Get the current time step, across the whole batch\n",
        "            context, attention_weights = self.attention(\n",
        "                h_prev, annotations, annotations\n",
        "            )  # batch_size x 1 x hidden_size\n",
        "            embed_and_context = torch.cat(\n",
        "                [embed_current, context.squeeze(1)], dim=1\n",
        "            )  # batch_size x (2*hidden_size)\n",
        "            h_prev = self.rnn(embed_and_context, h_prev)  # batch_size x hidden_size\n",
        "\n",
        "            hiddens.append(h_prev)\n",
        "            attentions.append(attention_weights)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1)  # batch_size x seq_len x hidden_size\n",
        "        attentions = torch.cat(attentions, dim=2)  # batch_size x seq_len x seq_len\n",
        "\n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, attentions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYPae08Io1Fi"
      },
      "source": [
        "## Step 3: Training and analysis (with additive attention)\n",
        "\n",
        "Now, run the following cell to train our recurrent encoder-decoder model with additive attention. How does it perform compared to the recurrent encoder-decoder model without attention?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ke6t6rCezpZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea34a688-0baf-466b-9f23-97564a5f2840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 10                                     \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn_attention                          \n",
            "                         attention_type: additive                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('insult', 'insultway')\n",
            "('march', 'archmay')\n",
            "('to', 'otay')\n",
            "('prove', 'ovepray')\n",
            "('soft', 'oftsay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.321 | Val loss: 2.005 | Gen: edededededededededed ingay-ingay-ingay-in ingay-ingay-ingay-in inssssssssssssssssss ingay-ingay-ingay-in\n",
            "Epoch:   1 | Train loss: 1.795 | Val loss: 1.807 | Gen: ay-oreday-oreday-ore arsay-iay-iay-iay-ia ingingingay-ingingin isisisisay illway-ingway-ingway\n",
            "Epoch:   2 | Train loss: 1.565 | Val loss: 1.684 | Gen: ay-eday-oreday-oreda illay-ilay-ilay-ilay ingingay-ingay-ingay ississay illway-orday-orty-ay\n",
            "Epoch:   3 | Train loss: 1.408 | Val loss: 1.656 | Gen: ededay-elleday-elede illway ingioningingingingin ississisessay ingpray-ingway-ingwa\n",
            "Epoch:   4 | Train loss: 1.297 | Val loss: 1.463 | Gen: erfay-elleway arway ondingingingway issississay ortingway\n",
            "Epoch:   5 | Train loss: 1.148 | Val loss: 1.455 | Gen: elfay-orgay-oryway away ondingingway ississingway ingway\n",
            "Epoch:   6 | Train loss: 1.035 | Val loss: 1.432 | Gen: ay-ay iray ondinginday issisway ingway\n",
            "Epoch:   7 | Train loss: 0.943 | Val loss: 1.132 | Gen: ehtay aringway ondininingingway isway oningway\n",
            "Epoch:   8 | Train loss: 0.792 | Val loss: 1.154 | Gen: ehthay iray ondiniondingnay issay omingway\n",
            "Epoch:   9 | Train loss: 0.751 | Val loss: 1.142 | Gen: etheray iraringay ondindindinioniondin isay orkingway\n",
            "Epoch:  10 | Train loss: 0.678 | Val loss: 1.070 | Gen: ehay airway ondicionionionday-oi isway orkingway\n",
            "Epoch:  11 | Train loss: 0.614 | Val loss: 0.980 | Gen: ehay airway ondiningay isway orkingway\n",
            "Epoch:  12 | Train loss: 0.512 | Val loss: 0.997 | Gen: ehay awaringway ondindiningoningunwa isway orkingsway\n",
            "Epoch:  13 | Train loss: 0.413 | Val loss: 0.726 | Gen: ehay airay ondicay isway orkingway\n",
            "Epoch:  14 | Train loss: 0.357 | Val loss: 0.790 | Gen: eway airway ondiiniondcay isay orkingingway\n",
            "Epoch:  15 | Train loss: 0.359 | Val loss: 0.777 | Gen: ethay awray onitiningcay isway orkingway\n",
            "Epoch:  16 | Train loss: 0.283 | Val loss: 0.612 | Gen: ehay airay ondiitingay isway orkingway\n",
            "Epoch:  17 | Train loss: 0.223 | Val loss: 0.704 | Gen: ethay airway onditioninininininin isway oringway\n",
            "Epoch:  18 | Train loss: 0.252 | Val loss: 1.001 | Gen: ethay airay ondiningcay isway orsigway\n",
            "Epoch:  19 | Train loss: 0.252 | Val loss: 0.663 | Gen: ethay airway ondiongcay issay orkingway\n",
            "Epoch:  20 | Train loss: 0.217 | Val loss: 0.760 | Gen: eway aiay onitionscay isway oringway\n",
            "Epoch:  21 | Train loss: 0.189 | Val loss: 0.790 | Gen: ehtay airray onditioncay issay orkingway\n",
            "Epoch:  22 | Train loss: 0.204 | Val loss: 0.627 | Gen: ethay airmay onditioncay isway othingway\n",
            "Epoch:  23 | Train loss: 0.225 | Val loss: 0.490 | Gen: etay airway onditioningay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.142 | Val loss: 0.441 | Gen: etay airway onditionionscay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.104 | Val loss: 0.461 | Gen: ethay airway ondititionscay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.122 | Val loss: 0.454 | Gen: etay away onditioncay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.130 | Val loss: 0.921 | Gen: etay airway onioningcay isway orynigway\n",
            "Epoch:  28 | Train loss: 0.228 | Val loss: 0.564 | Gen: etay awray onditionway isway orkingway\n",
            "Epoch:  29 | Train loss: 0.224 | Val loss: 0.889 | Gen: ehtay airway onditionscatingcatin isisway owkingway\n",
            "Epoch:  30 | Train loss: 0.201 | Val loss: 0.387 | Gen: etay airrway onditioncay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.104 | Val loss: 0.341 | Gen: ethay airway onditionscay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.061 | Val loss: 0.258 | Gen: ethay airway onditionscay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.043 | Val loss: 0.263 | Gen: ehay airway onditionscay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.037 | Val loss: 0.245 | Gen: ethay airway onditionscay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.037 | Val loss: 0.372 | Gen: ethay airway onditionsingcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.041 | Val loss: 0.237 | Gen: etay airway onditionscay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.026 | Val loss: 0.253 | Gen: etay airway onditionscay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.019 | Val loss: 0.227 | Gen: ethay airway onditionscay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.016 | Val loss: 0.220 | Gen: ethay airway onditionicynay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.014 | Val loss: 0.218 | Gen: ethay airway onditionicynay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.013 | Val loss: 0.224 | Gen: ethay airway onditionicynay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.018 | Val loss: 0.255 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.045 | Val loss: 0.573 | Gen: etay away onditioningcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.170 | Val loss: 1.246 | Gen: etay-ottay airray onddintitaninadinani isway orkway\n",
            "Epoch:  45 | Train loss: 0.272 | Val loss: 0.704 | Gen: ethay away onidionscay isway origway\n",
            "Epoch:  46 | Train loss: 0.143 | Val loss: 0.466 | Gen: eway awray onditionscay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.079 | Val loss: 0.334 | Gen: ethay awway onditioncay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.079 | Val loss: 0.587 | Gen: ethay airray onditioncay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.117 | Val loss: 0.328 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Obtained lowest validation loss of: 0.21774457288784502\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "rnn_attn_args = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_small\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 50,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"early_stopping_patience\": 10,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 32,\n",
        "    \"encoder_type\": \"rnn\",            # options: rnn / transformer\n",
        "    \"decoder_type\": \"rnn_attention\",  # options: rnn / rnn_attention / transformer\n",
        "    \"attention_type\": \"additive\",     # options: additive / scaled_dot\n",
        "}\n",
        "rnn_attn_args.update(args_dict)\n",
        "\n",
        "print_opts(rnn_attn_args)\n",
        "rnn_attn_encoder, rnn_attn_decoder, rnn_attn_losses = train(rnn_attn_args)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNVKbLc0ACj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9455c2d7-b042-49e2-fee9-8aa82944477f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditingcay isway orkingway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additive Attention:\n",
        "\n",
        "3: The model takes more time to train in each epoch compare to previous one. Because we adding more connections to the model so it take more time to train each epoch."
      ],
      "metadata": {
        "id": "9mXUs_fv7AQT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq7nhsEio1w-"
      },
      "source": [
        "## Step 4: Implement scaled dot-product attention\n",
        "\n",
        "In the next cell, you will implement the [scaled dot-product attention](https://paperswithcode.com/method/scaled) mechanism. See the assignment handouts for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_j3oY3hqsJQ"
      },
      "outputs": [],
      "source": [
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(\n",
        "            torch.tensor(self.hidden_size, dtype=torch.float)\n",
        "        )\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size = keys.size(0)\n",
        "        q = self.Q(queries)\n",
        "        k = self.K(keys)\n",
        "        v = self.V(values)\n",
        "        unnormalized_attention = torch.bmm(k, q.transpose(2,1))*self.scaling_factor\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = attention_weights.transpose(2,1) @ v\n",
        "        return context, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unReAOrjo113"
      },
      "source": [
        "## Step 5: Implement causal dot-product Attention\n",
        "\n",
        "\n",
        "Now, implement the casual scaled dot-product attention mechanism. It will be very similar to your implementation for `ScaledDotAttention`. The additional step is to mask out the attention to future timesteps so this attention mechanism can be used in a decoder. See the assignment handouts for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovigzQffrKqj"
      },
      "outputs": [],
      "source": [
        "class CausalScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CausalScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.neg_inf = torch.tensor(-1e7)\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(\n",
        "            torch.tensor(self.hidden_size, dtype=torch.float)\n",
        "        )\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        # batch_size = ...\n",
        "        batch_size = keys.size(0)   \n",
        "        q = self.Q(queries).view(batch_size, -1, self.hidden_size)\n",
        "        k = self.K(keys)\n",
        "        v = self.V(values)\n",
        "        unnormalized_attention = torch.bmm(k, q.transpose(2,1))*self.scaling_factor \n",
        "        mask = torch.tril(torch.ones_like(unnormalized_attention) * self.neg_inf, diagonal=-1)\n",
        "\n",
        "        attention_weights = self.softmax(unnormalized_attention + mask)\n",
        "        context = torch.bmm(attention_weights.transpose(2,1), v)\n",
        "        return context, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkjHbtvT6Qxs"
      },
      "source": [
        "## Step 6: Attention encoder and decoder\n",
        "\n",
        "The following cells provide an implementation of an encoder and decoder that use a single `ScaledDotAttention` block. Please read through them to understand what they are doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKGNqUaX6RLO"
      },
      "outputs": [],
      "source": [
        "class AttentionEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(AttentionEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.self_attention = ScaledDotAttention(\n",
        "                    hidden_size=hidden_size,\n",
        "                )\n",
        "               \n",
        "        self.attention_mlp = nn.Sequential(\n",
        "                                nn.Linear(hidden_size, hidden_size),\n",
        "                                nn.ReLU(),\n",
        "                              )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder scaled dot attention.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            None: Used to conform to standard encoder return signature.\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = inputs.size()\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        annotations = encoded\n",
        "        new_annotations, self_attention_weights = self.self_attention(\n",
        "            annotations, annotations, annotations\n",
        "        )  # batch_size x seq_len x hidden_size\n",
        "        residual_annotations = annotations + new_annotations\n",
        "        new_annotations = self.attention_mlp(residual_annotations)\n",
        "        annotations = residual_annotations + new_annotations\n",
        "\n",
        "        return annotations, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDUvtOee7cMy"
      },
      "outputs": [],
      "source": [
        "class AttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(AttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.self_attention = CausalScaledDotAttention(\n",
        "                                hidden_size=hidden_size,\n",
        "                                )\n",
        "                \n",
        "        self.decoder_attention = ScaledDotAttention(\n",
        "                                  hidden_size=hidden_size,\n",
        "                                  )\n",
        "                \n",
        "        self.attention_mlp = nn.Sequential(\n",
        "                                nn.Linear(hidden_size, hidden_size),\n",
        "                                nn.ReLU(),\n",
        "                              )\n",
        "                \n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        new_contexts, self_attention_weights = self.self_attention(\n",
        "            contexts, contexts, contexts\n",
        "        )  # batch_size x seq_len x hidden_size\n",
        "        residual_contexts = contexts + new_contexts\n",
        "        new_contexts, encoder_attention_weights = self.decoder_attention(\n",
        "            residual_contexts, annotations, annotations\n",
        "        )  # batch_size x seq_len x hidden_size\n",
        "        residual_contexts = residual_contexts + new_contexts\n",
        "        new_contexts = self.attention_mlp(residual_contexts)\n",
        "        contexts = residual_contexts + new_contexts\n",
        "\n",
        "        encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "        self_attention_weights_list.append(self_attention_weights)\n",
        "\n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "\n",
        "        return output, (encoder_attention_weights, self_attention_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7gJLw5t_rnW"
      },
      "source": [
        "## Step 7: Training and analysis (single scaled dot-product attention block)\n",
        "\n",
        "Now, train the following model, with an encoder and decoder each composed a single `ScaledDotAttention` block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MOkZonC8T3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "367c6fbd-55db-4ab4-958a-2647bdba13a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 100                                    \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: attention                              \n",
            "                           decoder_type: attention                              \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('insult', 'insultway')\n",
            "('march', 'archmay')\n",
            "('to', 'otay')\n",
            "('prove', 'ovepray')\n",
            "('soft', 'oftsay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.878 | Val loss: 2.509 | Gen: ay ay ay ay ay      \n",
            "Epoch:   1 | Train loss: 2.289 | Val loss: 2.249 | Gen: ay ay innnnnnnnnnnnnnnnnnn ay ay\n",
            "Epoch:   2 | Train loss: 2.073 | Val loss: 2.097 | Gen: ay ay ingingingingingingin aisay ay\n",
            "Epoch:   3 | Train loss: 1.945 | Val loss: 1.992 | Gen: ay ilay ingingingingingingin isay ingay\n",
            "Epoch:   4 | Train loss: 1.852 | Val loss: 1.922 | Gen: ay ilay ingingingingingingin isisay ingay\n",
            "Epoch:   5 | Train loss: 1.779 | Val loss: 1.874 | Gen: atetetay ilay ingingingingingingin isisisisisisisisisis ingay\n",
            "Epoch:   6 | Train loss: 1.720 | Val loss: 1.838 | Gen: atetay ilay ingingingingingingin isisisisisisisisisis ingay\n",
            "Epoch:   7 | Train loss: 1.670 | Val loss: 1.807 | Gen: atetay ilay ingingingingingingin isisisisisisisisisis ingay\n",
            "Epoch:   8 | Train loss: 1.625 | Val loss: 1.782 | Gen: atetay ilay ingingingingingingin isisisisisisisisisis ingay\n",
            "Epoch:   9 | Train loss: 1.584 | Val loss: 1.759 | Gen: atetay aray ingingingingingingin isissisisisisisissis ingay\n",
            "Epoch:  10 | Train loss: 1.546 | Val loss: 1.740 | Gen: atehay aray ingingingingingingin issisisisissisisisis ioray\n",
            "Epoch:  11 | Train loss: 1.512 | Val loss: 1.723 | Gen: atehay aray ingingingingingingin issisisissisissisisi aray\n",
            "Epoch:  12 | Train loss: 1.482 | Val loss: 1.706 | Gen: atehay aray ingingingingingingin issisissisissisissis aray\n",
            "Epoch:  13 | Train loss: 1.454 | Val loss: 1.693 | Gen: atehay aray ingingingingay ississisississisissa aray\n",
            "Epoch:  14 | Train loss: 1.428 | Val loss: 1.683 | Gen: atehay aray ingingay issississisississay oray\n",
            "Epoch:  15 | Train loss: 1.405 | Val loss: 1.670 | Gen: atehay aray ingingay ississississay oray\n",
            "Epoch:  16 | Train loss: 1.382 | Val loss: 1.662 | Gen: atehay aray ingay ississississay oray\n",
            "Epoch:  17 | Train loss: 1.362 | Val loss: 1.653 | Gen: athay aray ingay issississay oray\n",
            "Epoch:  18 | Train loss: 1.343 | Val loss: 1.642 | Gen: athay aray ingay ississississay oragwagwagwagwagwagw\n",
            "Epoch:  19 | Train loss: 1.325 | Val loss: 1.633 | Gen: athay aray ingay ississississay oragwagwagwagwagwagw\n",
            "Epoch:  20 | Train loss: 1.308 | Val loss: 1.623 | Gen: athay aray ingay ississississay oragwagwagwagwagwagw\n",
            "Epoch:  21 | Train loss: 1.292 | Val loss: 1.615 | Gen: athay aray ingay ississississay oragwagwagwagwagwagw\n",
            "Epoch:  22 | Train loss: 1.278 | Val loss: 1.606 | Gen: athay ariay ingay ississississay oragwagwagwagwagwagw\n",
            "Epoch:  23 | Train loss: 1.264 | Val loss: 1.596 | Gen: athay ariay ingay ississississay oragwagwagwagwagwagw\n",
            "Epoch:  24 | Train loss: 1.251 | Val loss: 1.586 | Gen: athay ariay ingay issississisay oragwagwagwagwagwagw\n",
            "Epoch:  25 | Train loss: 1.238 | Val loss: 1.578 | Gen: athay ariay ingay ississississay oragwagwagwagwagwagw\n",
            "Epoch:  26 | Train loss: 1.225 | Val loss: 1.567 | Gen: athay ariay ingay issississay oragwagwagwagwagwagw\n",
            "Epoch:  27 | Train loss: 1.213 | Val loss: 1.556 | Gen: athay ariay ingingay issississay oragwagwagwagwagwagw\n",
            "Epoch:  28 | Train loss: 1.201 | Val loss: 1.548 | Gen: athay ariay ingay issississay oray\n",
            "Epoch:  29 | Train loss: 1.190 | Val loss: 1.537 | Gen: athay ariay ingay ississisway oray\n",
            "Epoch:  30 | Train loss: 1.179 | Val loss: 1.527 | Gen: athay ariay ingay ississisway oray\n",
            "Epoch:  31 | Train loss: 1.168 | Val loss: 1.519 | Gen: athay ariay ingay ississway oray\n",
            "Epoch:  32 | Train loss: 1.158 | Val loss: 1.508 | Gen: athay ariay ingay ississway oray\n",
            "Epoch:  33 | Train loss: 1.147 | Val loss: 1.496 | Gen: athay ariay ingay ississway oray\n",
            "Epoch:  34 | Train loss: 1.137 | Val loss: 1.488 | Gen: ehthay ariay ingay ississway oray\n",
            "Epoch:  35 | Train loss: 1.127 | Val loss: 1.478 | Gen: ehthay ariay ingay ississway oray\n",
            "Epoch:  36 | Train loss: 1.117 | Val loss: 1.469 | Gen: ehthay ariay ingay ississway oray\n",
            "Epoch:  37 | Train loss: 1.108 | Val loss: 1.460 | Gen: ehthay ariay ingay issisway oray\n",
            "Epoch:  38 | Train loss: 1.098 | Val loss: 1.452 | Gen: ehthay ariay ingay issisway oray\n",
            "Epoch:  39 | Train loss: 1.089 | Val loss: 1.442 | Gen: ehthay ariay ingay issisway oray\n",
            "Epoch:  40 | Train loss: 1.080 | Val loss: 1.435 | Gen: ehthay ariay ingay issisway oray\n",
            "Epoch:  41 | Train loss: 1.072 | Val loss: 1.426 | Gen: ehthay ariay ingay issway oray\n",
            "Epoch:  42 | Train loss: 1.063 | Val loss: 1.418 | Gen: ehthay ariay ingay issway oray\n",
            "Epoch:  43 | Train loss: 1.054 | Val loss: 1.411 | Gen: ehthay ariay ingay issway oray\n",
            "Epoch:  44 | Train loss: 1.046 | Val loss: 1.405 | Gen: ehthay ariay ingay issway oray\n",
            "Epoch:  45 | Train loss: 1.038 | Val loss: 1.396 | Gen: ehthay ariay ingioingway issway oray\n",
            "Epoch:  46 | Train loss: 1.029 | Val loss: 1.387 | Gen: ehthay ariaway ingioingway issway oray\n",
            "Epoch:  47 | Train loss: 1.021 | Val loss: 1.381 | Gen: ehthay ariaway ingioingway issway oray\n",
            "Epoch:  48 | Train loss: 1.014 | Val loss: 1.374 | Gen: ehthay ariaway ingioingway issway oray\n",
            "Epoch:  49 | Train loss: 1.007 | Val loss: 1.369 | Gen: ehthay ariaway ingioingway issway oray\n",
            "Epoch:  50 | Train loss: 1.000 | Val loss: 1.363 | Gen: ehthay ariaway ingioingway issway orway\n",
            "Epoch:  51 | Train loss: 0.993 | Val loss: 1.361 | Gen: ehthay ariaway ingioingway issway orway\n",
            "Epoch:  52 | Train loss: 0.987 | Val loss: 1.355 | Gen: ehthay ariaway ingioingway issway orway\n",
            "Epoch:  53 | Train loss: 0.981 | Val loss: 1.352 | Gen: ehthay ariway ingioingway issway orway\n",
            "Epoch:  54 | Train loss: 0.976 | Val loss: 1.347 | Gen: ehthay ariway ingioingway issway orway\n",
            "Epoch:  55 | Train loss: 0.970 | Val loss: 1.345 | Gen: ehthay ariway ingiongway issway orway\n",
            "Epoch:  56 | Train loss: 0.965 | Val loss: 1.340 | Gen: ehthay ariway ingiongway issway orway\n",
            "Epoch:  57 | Train loss: 0.960 | Val loss: 1.338 | Gen: ehthay ariway ingiongway issway orway\n",
            "Epoch:  58 | Train loss: 0.955 | Val loss: 1.333 | Gen: ehthay ariway ingiongway issway orway\n",
            "Epoch:  59 | Train loss: 0.950 | Val loss: 1.331 | Gen: ehthay ariway ingiongway issway orway\n",
            "Epoch:  60 | Train loss: 0.945 | Val loss: 1.327 | Gen: ehthay ariway ingiongway issway orway\n",
            "Epoch:  61 | Train loss: 0.940 | Val loss: 1.325 | Gen: ehthay ariway ingiongway issway orway\n",
            "Epoch:  62 | Train loss: 0.936 | Val loss: 1.322 | Gen: ehthay ariway ingiongway issway orway\n",
            "Epoch:  63 | Train loss: 0.931 | Val loss: 1.320 | Gen: ehthay ariway ingiongway issway orway\n",
            "Epoch:  64 | Train loss: 0.927 | Val loss: 1.316 | Gen: ehthay ariway ingiongway issway oringwwway\n",
            "Epoch:  65 | Train loss: 0.923 | Val loss: 1.315 | Gen: ehthay ariway ingiongway issway oringwwwway\n",
            "Epoch:  66 | Train loss: 0.919 | Val loss: 1.312 | Gen: ehthay ariway ingiongway issway oringwwway\n",
            "Epoch:  67 | Train loss: 0.915 | Val loss: 1.310 | Gen: ehthay ariway ingiongway issway oringwwway\n",
            "Epoch:  68 | Train loss: 0.911 | Val loss: 1.307 | Gen: ehthay ariway ingiongway issway oringwwway\n",
            "Epoch:  69 | Train loss: 0.907 | Val loss: 1.306 | Gen: ehthay ariway ingiongway issway owlway\n",
            "Epoch:  70 | Train loss: 0.904 | Val loss: 1.303 | Gen: ehthay ariway ingiongway issway owlway\n",
            "Epoch:  71 | Train loss: 0.900 | Val loss: 1.301 | Gen: ehthay ariway ingiongway issway oringwwway\n",
            "Epoch:  72 | Train loss: 0.896 | Val loss: 1.299 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  73 | Train loss: 0.893 | Val loss: 1.297 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  74 | Train loss: 0.890 | Val loss: 1.295 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  75 | Train loss: 0.886 | Val loss: 1.293 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  76 | Train loss: 0.883 | Val loss: 1.290 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  77 | Train loss: 0.880 | Val loss: 1.289 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  78 | Train loss: 0.877 | Val loss: 1.287 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  79 | Train loss: 0.874 | Val loss: 1.285 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  80 | Train loss: 0.871 | Val loss: 1.283 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  81 | Train loss: 0.868 | Val loss: 1.282 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  82 | Train loss: 0.865 | Val loss: 1.280 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  83 | Train loss: 0.862 | Val loss: 1.278 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  84 | Train loss: 0.860 | Val loss: 1.277 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  85 | Train loss: 0.857 | Val loss: 1.275 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  86 | Train loss: 0.854 | Val loss: 1.274 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  87 | Train loss: 0.851 | Val loss: 1.272 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  88 | Train loss: 0.849 | Val loss: 1.271 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  89 | Train loss: 0.846 | Val loss: 1.269 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  90 | Train loss: 0.844 | Val loss: 1.268 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  91 | Train loss: 0.842 | Val loss: 1.267 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  92 | Train loss: 0.839 | Val loss: 1.266 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  93 | Train loss: 0.837 | Val loss: 1.265 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  94 | Train loss: 0.835 | Val loss: 1.264 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  95 | Train loss: 0.832 | Val loss: 1.263 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  96 | Train loss: 0.830 | Val loss: 1.262 | Gen: ehthay ariway ingiontingway issway owlway\n",
            "Epoch:  97 | Train loss: 0.828 | Val loss: 1.261 | Gen: ehthay ariway ingintiongway issway owlway\n",
            "Epoch:  98 | Train loss: 0.826 | Val loss: 1.260 | Gen: ehthay ariway ingintiongway issway owlway\n",
            "Epoch:  99 | Train loss: 0.824 | Val loss: 1.259 | Gen: ehthay ariway ingintiongway issway owlway\n",
            "Obtained lowest validation loss of: 1.2589281337416691\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehthay ariway ingintiongway issway owlway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "attention_args_s = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_small\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 100,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 100,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 32,\n",
        "    \"encoder_type\": \"attention\",\n",
        "    \"decoder_type\": \"attention\",  # options: rnn / rnn_attention / attention / transformer\n",
        "}\n",
        "attention_args_s.update(args_dict)\n",
        "print_opts(attention_args_s)\n",
        "\n",
        "attention_encoder_s, attention_decoder_s, attention_losses_s = train(attention_args_s)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, attention_encoder_s, attention_decoder_s, None, attention_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tcpUFKqo2Oi"
      },
      "source": [
        "## Step 8: Transformer encoder and decoder\n",
        "\n",
        "The following cells provide an implementation of the transformer encoder and decoder that use your `ScaledDotAttention` and `CausalScaledDotAttention`. Please read through them to understand what they are doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3B-fWsarlVk"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers, opts):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.self_attentions = nn.ModuleList(\n",
        "            [\n",
        "                ScaledDotAttention(\n",
        "                    hidden_size=hidden_size,\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.attention_mlps = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(hidden_size, hidden_size),\n",
        "                    nn.ReLU(),\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            None: Used to conform to standard encoder return signature.\n",
        "            None: Used to conform to standard encoder return signature.\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = inputs.size()\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        # Add positinal embeddings from self.create_positional_encodings. (a'la https://arxiv.org/pdf/1706.03762.pdf, section 3.5)\n",
        "        encoded = encoded + self.positional_encodings[:seq_len]\n",
        "\n",
        "        annotations = encoded\n",
        "        for i in range(self.num_layers):\n",
        "            new_annotations, self_attention_weights = self.self_attentions[i](\n",
        "                annotations, annotations, annotations\n",
        "            )  # batch_size x seq_len x hidden_size\n",
        "            residual_annotations = annotations + new_annotations\n",
        "            new_annotations = self.attention_mlps[i](residual_annotations)\n",
        "            annotations = residual_annotations + new_annotations\n",
        "\n",
        "        # Transformer encoder does not have a last hidden or cell layer.\n",
        "        return annotations, None\n",
        "        # return annotations, None, None\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "        \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "        Arguments:\n",
        "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "        Returns:\n",
        "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len.\n",
        "        \"\"\"\n",
        "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "        dim_indices = torch.arange(self.hidden_size // 2)[None, ...]\n",
        "        exponents = (2 * dim_indices).float() / (self.hidden_size)\n",
        "        trig_args = pos_indices / (10000**exponents)\n",
        "        sin_terms = torch.sin(trig_args)\n",
        "        cos_terms = torch.cos(trig_args)\n",
        "\n",
        "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "        pos_encodings[:, 0::2] = sin_terms\n",
        "        pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "        if self.opts.cuda:\n",
        "            pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "        return pos_encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyvTZFxtrvc6"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.self_attentions = nn.ModuleList(\n",
        "            [\n",
        "                CausalScaledDotAttention(\n",
        "                    hidden_size=hidden_size,\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.encoder_attentions = nn.ModuleList(\n",
        "            [\n",
        "                ScaledDotAttention(\n",
        "                    hidden_size=hidden_size,\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.attention_mlps = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(hidden_size, hidden_size),\n",
        "                    nn.ReLU(),\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        embed = embed + self.positional_encodings[:seq_len]\n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        for i in range(self.num_layers):\n",
        "            new_contexts, self_attention_weights = self.self_attentions[i](\n",
        "                contexts, contexts, contexts\n",
        "            )  # batch_size x seq_len x hidden_size\n",
        "            residual_contexts = contexts + new_contexts\n",
        "            new_contexts, encoder_attention_weights = self.encoder_attentions[i](\n",
        "                residual_contexts, annotations, annotations\n",
        "            )  # batch_size x seq_len x hidden_size\n",
        "            residual_contexts = residual_contexts + new_contexts\n",
        "            new_contexts = self.attention_mlps[i](residual_contexts)\n",
        "            contexts = residual_contexts + new_contexts\n",
        "\n",
        "            encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "            self_attention_weights_list.append(self_attention_weights)\n",
        "\n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "\n",
        "        return output, (encoder_attention_weights, self_attention_weights)\n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "        \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "        Arguments:\n",
        "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "        Returns:\n",
        "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len.\n",
        "        \"\"\"\n",
        "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "        dim_indices = torch.arange(self.hidden_size // 2)[None, ...]\n",
        "        exponents = (2 * dim_indices).float() / (self.hidden_size)\n",
        "        trig_args = pos_indices / (10000**exponents)\n",
        "        sin_terms = torch.sin(trig_args)\n",
        "        cos_terms = torch.cos(trig_args)\n",
        "\n",
        "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "        pos_encodings[:, 0::2] = sin_terms\n",
        "        pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "        pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "        return pos_encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29ZjkXTNrUKb"
      },
      "source": [
        "\n",
        "## Step 9: Training and analysis (with scaled dot-product attention)\n",
        "\n",
        "Now we will train a (simplified) transformer encoder-decoder model.\n",
        "\n",
        "First, we train our smaller model on the small dataset. Use this model to answer Question 5 in the handout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk8e4KSnuZ8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f21258d3-4e37-4ed0-b5aa-18c26dd7fc3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 100                                    \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 4                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('insult', 'insultway')\n",
            "('march', 'archmay')\n",
            "('to', 'otay')\n",
            "('prove', 'ovepray')\n",
            "('soft', 'oftsay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.960 | Val loss: 2.385 | Gen: ay ay elnininoay iy inday-ay\n",
            "Epoch:   1 | Train loss: 2.098 | Val loss: 2.044 | Gen: eday ay inolningday iway ingway\n",
            "Epoch:   2 | Train loss: 1.847 | Val loss: 1.877 | Gen: eay ay inonsinongway iway ingway-inway\n",
            "Epoch:   3 | Train loss: 1.679 | Val loss: 1.784 | Gen: eay ay inssssingtingggggggg issway ingway-ongway\n",
            "Epoch:   4 | Train loss: 1.548 | Val loss: 1.735 | Gen: eway iway inongingtingway issway ingway-ongway-ongy\n",
            "Epoch:   5 | Train loss: 1.454 | Val loss: 1.674 | Gen: etay iway inongingway-inongy issway ingway-ongway-oway\n",
            "Epoch:   6 | Train loss: 1.379 | Val loss: 1.619 | Gen: eway iway indinginday isway ingway-inday\n",
            "Epoch:   7 | Train loss: 1.310 | Val loss: 1.657 | Gen: eway iway indingindedingway iway ingway-way\n",
            "Epoch:   8 | Train loss: 1.266 | Val loss: 1.707 | Gen: ethay iway elintintingtingtingt issway oringway-way-y\n",
            "Epoch:   9 | Train loss: 1.218 | Val loss: 1.535 | Gen: ethay iway oindingtindingtingti issway orgringway\n",
            "Epoch:  10 | Train loss: 1.145 | Val loss: 1.415 | Gen: eway iway oinginginday isway oghay-ingway\n",
            "Epoch:  11 | Train loss: 1.108 | Val loss: 1.530 | Gen: etay iway ongingindingay isway inggway-gay\n",
            "Epoch:  12 | Train loss: 1.065 | Val loss: 1.452 | Gen: ethay iway ongingway isway onghway\n",
            "Epoch:  13 | Train loss: 1.023 | Val loss: 1.517 | Gen: etay arway ondincay isway oray-oway\n",
            "Epoch:  14 | Train loss: 0.994 | Val loss: 1.406 | Gen: ehay iway ondingatindedngay isway orarwiway\n",
            "Epoch:  15 | Train loss: 0.922 | Val loss: 1.488 | Gen: ehay iway oniintinday isway oraringway\n",
            "Epoch:  16 | Train loss: 0.887 | Val loss: 1.296 | Gen: ehay iway ondinintingay isay oringway\n",
            "Epoch:  17 | Train loss: 0.871 | Val loss: 1.453 | Gen: ethay araray ononinintindatindnay isway oringway\n",
            "Epoch:  18 | Train loss: 0.858 | Val loss: 1.411 | Gen: ethay iway ondindinday isway orarwingway\n",
            "Epoch:  19 | Train loss: 0.830 | Val loss: 1.390 | Gen: ehay irway ondinsintinday isway oringrway\n",
            "Epoch:  20 | Train loss: 0.813 | Val loss: 1.377 | Gen: ehehay irarway ondinincingtingggtin isway oringrway\n",
            "Epoch:  21 | Train loss: 0.761 | Val loss: 1.255 | Gen: ehay iway ondindintingcay isway onghingway\n",
            "Epoch:  22 | Train loss: 0.720 | Val loss: 1.273 | Gen: ehay iway ondinintingciongggti isway oringiway\n",
            "Epoch:  23 | Train loss: 0.689 | Val loss: 1.245 | Gen: ehay iway onininintindngay isway ongringway\n",
            "Epoch:  24 | Train loss: 0.658 | Val loss: 1.201 | Gen: ehyhay iway ondinintingctionggng isway oringrway\n",
            "Epoch:  25 | Train loss: 0.632 | Val loss: 1.227 | Gen: ekhy irarray oninedinintingtingns isway oningriway\n",
            "Epoch:  26 | Train loss: 0.608 | Val loss: 1.254 | Gen: ehay iway oninediningtingway isway oringingway\n",
            "Epoch:  27 | Train loss: 0.637 | Val loss: 1.225 | Gen: ethay airway ondinincingggggggnsn isway ongringway\n",
            "Epoch:  28 | Train loss: 0.662 | Val loss: 1.358 | Gen: eway airray ondininindinday ssay oringraway\n",
            "Epoch:  29 | Train loss: 0.771 | Val loss: 1.225 | Gen: ekehay irway ondinesingcay isway oringhingway\n",
            "Epoch:  30 | Train loss: 0.627 | Val loss: 1.094 | Gen: ethay iway ondiintingay isway oringiway\n",
            "Epoch:  31 | Train loss: 0.538 | Val loss: 1.100 | Gen: ehay airay onintingay isway oringhiway\n",
            "Epoch:  32 | Train loss: 0.497 | Val loss: 1.100 | Gen: ethay airay onintingay isway oringiway\n",
            "Epoch:  33 | Train loss: 0.468 | Val loss: 1.107 | Gen: ethay airaray onintingay isway oringhiway\n",
            "Epoch:  34 | Train loss: 0.447 | Val loss: 1.082 | Gen: ethay airaray onintingay isway oringiway\n",
            "Epoch:  35 | Train loss: 0.427 | Val loss: 1.126 | Gen: ethay airaray onintingay isway oringhiway\n",
            "Epoch:  36 | Train loss: 0.410 | Val loss: 1.103 | Gen: ethay airaray onintitingcay isway oringiway\n",
            "Epoch:  37 | Train loss: 0.394 | Val loss: 1.145 | Gen: ethay airaray onintingcay isway owghingway\n",
            "Epoch:  38 | Train loss: 0.382 | Val loss: 1.126 | Gen: ethay airaray onintitingcay isway okingragway\n",
            "Epoch:  39 | Train loss: 0.366 | Val loss: 1.125 | Gen: ethay airaray onintitingcay isway okingraway\n",
            "Epoch:  40 | Train loss: 0.355 | Val loss: 1.169 | Gen: ethay airaray onintitingcay isway okingraway\n",
            "Epoch:  41 | Train loss: 0.357 | Val loss: 1.157 | Gen: ethay airaray onititingcay isway owiingway\n",
            "Epoch:  42 | Train loss: 0.392 | Val loss: 1.199 | Gen: ehay airaray oninditingcay isway okingrway\n",
            "Epoch:  43 | Train loss: 0.557 | Val loss: 1.194 | Gen: ethay airway onditingcay isway owgringway\n",
            "Epoch:  44 | Train loss: 0.519 | Val loss: 1.042 | Gen: ethay airway ondiitingcay isway okingway\n",
            "Epoch:  45 | Train loss: 0.376 | Val loss: 1.131 | Gen: ethay airway onintitiongcay isway okingway\n",
            "Epoch:  46 | Train loss: 0.331 | Val loss: 1.006 | Gen: ethay airway ondiitingcay isway okingway\n",
            "Epoch:  47 | Train loss: 0.328 | Val loss: 1.076 | Gen: ethay aiway onditingcay isway okingway\n",
            "Epoch:  48 | Train loss: 0.330 | Val loss: 1.055 | Gen: ethay airaray ondiitingcay isway okingway\n",
            "Epoch:  49 | Train loss: 0.295 | Val loss: 0.974 | Gen: ethay airay onditingcay isway okingway\n",
            "Epoch:  50 | Train loss: 0.275 | Val loss: 1.091 | Gen: ethay airay onintitingcay isway okingray\n",
            "Epoch:  51 | Train loss: 0.266 | Val loss: 1.000 | Gen: ethay airayway ondititingcay isway okingray\n",
            "Epoch:  52 | Train loss: 0.251 | Val loss: 0.986 | Gen: ethay airaray ondititingcay isway okingray\n",
            "Epoch:  53 | Train loss: 0.242 | Val loss: 1.029 | Gen: ethay airay ondititingcay isway okingway\n",
            "Epoch:  54 | Train loss: 0.234 | Val loss: 1.012 | Gen: ethay airaray ondititingcay isway okingray\n",
            "Epoch:  55 | Train loss: 0.227 | Val loss: 1.025 | Gen: ethay airayway onintitingcay isway okingray\n",
            "Epoch:  56 | Train loss: 0.222 | Val loss: 1.033 | Gen: ethay airaray ondititingcay isway okingiway\n",
            "Epoch:  57 | Train loss: 0.215 | Val loss: 1.039 | Gen: ethay airaray onititingcay isway okingray\n",
            "Epoch:  58 | Train loss: 0.211 | Val loss: 1.048 | Gen: ethay ariway ondititingcay isway okingray\n",
            "Epoch:  59 | Train loss: 0.209 | Val loss: 1.070 | Gen: ethay airway onititingcay isway okingray\n",
            "Epoch:  60 | Train loss: 0.209 | Val loss: 1.048 | Gen: ethay airaray onditiningcay isway okingray\n",
            "Epoch:  61 | Train loss: 0.214 | Val loss: 1.184 | Gen: ethay airayway ondititiongcay isway okingray\n",
            "Epoch:  62 | Train loss: 0.270 | Val loss: 1.187 | Gen: ethay aiwayway onditindingcay isway okingsway\n",
            "Epoch:  63 | Train loss: 0.290 | Val loss: 1.090 | Gen: ethay airaway onititindcay isway okingway\n",
            "Epoch:  64 | Train loss: 0.282 | Val loss: 1.076 | Gen: ethay ariway oninitingcay isway owingway\n",
            "Epoch:  65 | Train loss: 0.245 | Val loss: 1.139 | Gen: ethay ariray ondiitiningcay isway oringiway\n",
            "Epoch:  66 | Train loss: 0.227 | Val loss: 0.967 | Gen: ethay ariway onditiningcay isway owingway\n",
            "Epoch:  67 | Train loss: 0.197 | Val loss: 1.000 | Gen: ethay ariway onditiiningcay isway okingray\n",
            "Epoch:  68 | Train loss: 0.178 | Val loss: 0.969 | Gen: ethay ariway ondititingcay isway okingray\n",
            "Epoch:  69 | Train loss: 0.167 | Val loss: 0.987 | Gen: ethay ariway ondititingcay isway okingray\n",
            "Epoch:  70 | Train loss: 0.161 | Val loss: 0.985 | Gen: ethay ariway onditiningcay isway okingway\n",
            "Epoch:  71 | Train loss: 0.154 | Val loss: 1.004 | Gen: ethay ariway onditiingcay isway okingray\n",
            "Epoch:  72 | Train loss: 0.150 | Val loss: 1.007 | Gen: ethay ariway onditiningcay isway okingway\n",
            "Epoch:  73 | Train loss: 0.143 | Val loss: 1.020 | Gen: ethay ariway onditiningcay isway okingway\n",
            "Epoch:  74 | Train loss: 0.140 | Val loss: 1.029 | Gen: ethay ariway onditiningcay isway okingway\n",
            "Epoch:  75 | Train loss: 0.134 | Val loss: 1.039 | Gen: ethay ariway onditiningcay isway okingway\n",
            "Epoch:  76 | Train loss: 0.132 | Val loss: 1.062 | Gen: ethay ariway onditiningcay isway okingway\n",
            "Epoch:  77 | Train loss: 0.128 | Val loss: 1.056 | Gen: ethay ariway onditiningcay isway okingway\n",
            "Epoch:  78 | Train loss: 0.127 | Val loss: 1.093 | Gen: ethay ariway onditingcay isway okingway\n",
            "Epoch:  79 | Train loss: 0.121 | Val loss: 1.079 | Gen: ethay ariway onditiningcay isway okingway\n",
            "Epoch:  80 | Train loss: 0.120 | Val loss: 1.092 | Gen: ethay ariway onditiningcay isway okingway\n",
            "Epoch:  81 | Train loss: 0.114 | Val loss: 1.098 | Gen: ethay ariway onditiningcay isway okingway\n",
            "Epoch:  82 | Train loss: 0.115 | Val loss: 1.136 | Gen: ethay ariway onditingcay isway okingway\n",
            "Epoch:  83 | Train loss: 0.109 | Val loss: 1.137 | Gen: ethay ariway onditiningcay isway okingway\n",
            "Epoch:  84 | Train loss: 0.110 | Val loss: 1.169 | Gen: ethay ariway onditingcay isway okingway\n",
            "Epoch:  85 | Train loss: 0.113 | Val loss: 1.285 | Gen: ethay ariray onditindiongcay isway okingrway\n",
            "Epoch:  86 | Train loss: 0.153 | Val loss: 1.297 | Gen: ethay airway ondititingcay isway okigrway\n",
            "Epoch:  87 | Train loss: 0.191 | Val loss: 1.238 | Gen: ethay ariway ondititingcay isway okingrway\n",
            "Epoch:  88 | Train loss: 0.271 | Val loss: 1.263 | Gen: ethay airway onditiondiongay isway okingway\n",
            "Epoch:  89 | Train loss: 0.264 | Val loss: 1.015 | Gen: ethay ariway onditiningcay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.168 | Val loss: 0.976 | Gen: ethay ariway onditingcingway isway okingway\n",
            "Epoch:  91 | Train loss: 0.132 | Val loss: 0.989 | Gen: ethay ariway onditiningcay isway okingway\n",
            "Epoch:  92 | Train loss: 0.114 | Val loss: 0.999 | Gen: ethay ariway onditindingcay isway okingway\n",
            "Epoch:  93 | Train loss: 0.106 | Val loss: 1.002 | Gen: ethay ariway onditindongcay isway okingway\n",
            "Epoch:  94 | Train loss: 0.100 | Val loss: 1.013 | Gen: ethay ariway onditindongcay isway okingway\n",
            "Epoch:  95 | Train loss: 0.095 | Val loss: 1.024 | Gen: ethay ariway onditindongcay isway okingway\n",
            "Epoch:  96 | Train loss: 0.091 | Val loss: 1.036 | Gen: ethay ariway onditindongcay isway okingway\n",
            "Epoch:  97 | Train loss: 0.088 | Val loss: 1.047 | Gen: ethay ariway onditindongcay isway okingway\n",
            "Epoch:  98 | Train loss: 0.085 | Val loss: 1.059 | Gen: ethay ariway onditindongcay isway okingway\n",
            "Epoch:  99 | Train loss: 0.082 | Val loss: 1.067 | Gen: ethay ariway onditindongcay isway okingway\n",
            "Obtained lowest validation loss of: 0.9665226748835741\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay ariway onditindongcay isway okingway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "trans32_args_s = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_small\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 100,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 100,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 32,\n",
        "    \"encoder_type\": \"transformer\",\n",
        "    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n",
        "    \"num_transformer_layers\": 4,\n",
        "}\n",
        "trans32_args_s.update(args_dict)\n",
        "print_opts(trans32_args_s)\n",
        "\n",
        "trans32_encoder_s, trans32_decoder_s, trans32_losses_s = train(trans32_args_s)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l28mKuZxvaRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fca9861-175f-4f57-dde3-fa600c3fe6cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay ariway onditindongcay isway okingway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L8EqLYFu48H"
      },
      "source": [
        "In the following cells, we investigate the effects of increasing model size and dataset size on the training / validation curves and generalization of the Transformer. We will increase hidden size to 64, and also increase dataset size. Include the best achieved validation loss in your report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdZO69DozuUu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a7d2233-307e-4e38-e1bc-721dbb83cdd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 10                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('ninja', 'injanay')\n",
            "('iteration', 'iterationway')\n",
            "('chick', 'ickchay')\n",
            "('innovation', 'innovationway')\n",
            "('eclat', 'eclatway')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 3.122 | Val loss: 2.465 | Gen: ay aaay aay eway aaaaaaaaaaaaaay\n",
            "Epoch:   1 | Train loss: 2.191 | Val loss: 2.217 | Gen: ay ay ontay-ay-onay-onay-o ay onay-onay\n",
            "Epoch:   2 | Train loss: 1.939 | Val loss: 2.077 | Gen: ay away ontay-onontay-ononay away ontay-onay\n",
            "Epoch:   3 | Train loss: 1.773 | Val loss: 1.926 | Gen: tay away ontay-ontay-ononday isay ontay-onway\n",
            "Epoch:   4 | Train loss: 1.634 | Val loss: 1.845 | Gen: tay away ontay-onday-onday isay ontay-onray\n",
            "Epoch:   5 | Train loss: 1.528 | Val loss: 1.816 | Gen: etay away-ay ontinonontay-ononday isay ongingay-onsay\n",
            "Epoch:   6 | Train loss: 1.448 | Val loss: 1.723 | Gen: etay away ontinay-ononay isay orgrgay-ongway\n",
            "Epoch:   7 | Train loss: 1.385 | Val loss: 1.682 | Gen: eway away ontinay-onay-onday isay orgrgay-ingingway\n",
            "Epoch:   8 | Train loss: 1.331 | Val loss: 1.624 | Gen: etay away ontinginginay-onday isway orgrgay\n",
            "Epoch:   9 | Train loss: 1.262 | Val loss: 1.584 | Gen: ettay away ontinontinay-onday-o isway orgingray\n",
            "Epoch:  10 | Train loss: 1.199 | Val loss: 1.530 | Gen: etetay away ontinginginay-ony-on isway orgrgingray\n",
            "Epoch:  11 | Train loss: 1.139 | Val loss: 1.500 | Gen: etetay away onintinay-onay-onday isway orgingray\n",
            "Epoch:  12 | Train loss: 1.088 | Val loss: 1.454 | Gen: etetay arway onintinay-onay-onday isway orgingray\n",
            "Epoch:  13 | Train loss: 1.039 | Val loss: 1.441 | Gen: eteway away onintinginay-onay isway orgingway\n",
            "Epoch:  14 | Train loss: 1.001 | Val loss: 1.476 | Gen: etetay arway onintinginay-onay isway orgingray\n",
            "Epoch:  15 | Train loss: 0.965 | Val loss: 1.443 | Gen: ethay ay onintiongionay isy orgingway\n",
            "Epoch:  16 | Train loss: 0.948 | Val loss: 1.522 | Gen: ewhay arway oninionday-inay isway orgingray\n",
            "Epoch:  17 | Train loss: 0.951 | Val loss: 1.522 | Gen: ewhay away onindiongionay-ay isway orgingway\n",
            "Epoch:  18 | Train loss: 0.888 | Val loss: 1.471 | Gen: ewhay away onindiongionay isway orgingway\n",
            "Epoch:  19 | Train loss: 0.847 | Val loss: 1.516 | Gen: ethay arway ondiniongiongay isway orgingray\n",
            "Epoch:  20 | Train loss: 0.810 | Val loss: 1.556 | Gen: eway arway ondinionionginay isay ormringay\n",
            "Epoch:  21 | Train loss: 0.803 | Val loss: 1.532 | Gen: ethay arway ondiongiongingay isay orgingray\n",
            "Epoch:  22 | Train loss: 0.791 | Val loss: 1.574 | Gen: ewhay away ondioniongionay isway orway\n",
            "Epoch:  23 | Train loss: 0.797 | Val loss: 1.556 | Gen: ewhay arway oniotiongiongay isway orsingway\n",
            "Validation loss has not improved in 10 epochs, stopping early\n",
            "Obtained lowest validation loss of: 1.4409790778389344\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay ariay oniniotingionay isyway orgingway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "trans32_args_l = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_large\",  # Increased data set size\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 100,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 10,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 512,\n",
        "    \"hidden_size\": 32,\n",
        "    \"encoder_type\": \"transformer\",\n",
        "    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n",
        "    \"num_transformer_layers\": 3,\n",
        "}\n",
        "trans32_args_l.update(args_dict)\n",
        "print_opts(trans32_args_l)\n",
        "\n",
        "trans32_encoder_l, trans32_decoder_l, trans32_losses_l = train(trans32_args_l)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans32_encoder_l, trans32_decoder_l, None, trans32_args_l\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmoTgrDcr_dw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5e12726-3ef7-4ff8-9933-54f496ae2474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 20                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('insult', 'insultway')\n",
            "('march', 'archmay')\n",
            "('to', 'otay')\n",
            "('prove', 'ovepray')\n",
            "('soft', 'oftsay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.410 | Val loss: 2.040 | Gen: eteteteteteeetetetec iway ontingay isssssssssssssssssss oeway-ay\n",
            "Epoch:   1 | Train loss: 1.720 | Val loss: 1.860 | Gen: etway iway-iway ingnionioniongiongwa isisisisisisisisisis oingway\n",
            "Epoch:   2 | Train loss: 1.441 | Val loss: 1.603 | Gen: ethththhway ariway ondindintingay issisisay ongway\n",
            "Epoch:   3 | Train loss: 1.246 | Val loss: 1.648 | Gen: ehay away ondinatingway isisway ongway-y\n",
            "Epoch:   4 | Train loss: 1.150 | Val loss: 1.473 | Gen: eththay arway ondintinay isway ongway\n",
            "Epoch:   5 | Train loss: 1.008 | Val loss: 1.372 | Gen: ethay arway ondinationay isway oway\n",
            "Epoch:   6 | Train loss: 0.849 | Val loss: 1.257 | Gen: ehay arway onditonday isway ororkway\n",
            "Epoch:   7 | Train loss: 0.752 | Val loss: 1.184 | Gen: ehay arway ondiongdingway isway oringway\n",
            "Epoch:   8 | Train loss: 0.664 | Val loss: 1.203 | Gen: ethay arway ondiongionooooay isway oringway\n",
            "Epoch:   9 | Train loss: 0.581 | Val loss: 1.174 | Gen: ethethay arriay ondionitingiongway isay orkngway\n",
            "Epoch:  10 | Train loss: 0.549 | Val loss: 1.168 | Gen: ehay ariay onditiongway isway orkway\n",
            "Epoch:  11 | Train loss: 0.524 | Val loss: 1.241 | Gen: ethethay ariway onditiongtiongtay isway okringway\n",
            "Epoch:  12 | Train loss: 0.541 | Val loss: 1.185 | Gen: etheway ariay ondiifingway issway orkingway\n",
            "Epoch:  13 | Train loss: 0.433 | Val loss: 1.012 | Gen: ethay ariay onditiongdinay isway orkway\n",
            "Epoch:  14 | Train loss: 0.339 | Val loss: 0.927 | Gen: ethay ariway onditingway isway orkway\n",
            "Epoch:  15 | Train loss: 0.287 | Val loss: 0.905 | Gen: ethway ariway onditiongway isway orkingway\n",
            "Epoch:  16 | Train loss: 0.253 | Val loss: 0.994 | Gen: ethay ariway onditiongcay isway rrrrrkway\n",
            "Epoch:  17 | Train loss: 0.253 | Val loss: 0.891 | Gen: ethay airay onditiongway isway ookingway\n",
            "Epoch:  18 | Train loss: 0.237 | Val loss: 1.079 | Gen: ethway ariway onditiongcay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.313 | Val loss: 0.925 | Gen: ethay ariray onditiongcoway isway okraingway\n",
            "Epoch:  20 | Train loss: 0.264 | Val loss: 0.815 | Gen: ethway ariway onditiongway isway ookwgway\n",
            "Epoch:  21 | Train loss: 0.267 | Val loss: 1.072 | Gen: ethay awiray owititiongway isway owiingway\n",
            "Epoch:  22 | Train loss: 0.261 | Val loss: 0.964 | Gen: ethay array onditiongway isay orkway\n",
            "Epoch:  23 | Train loss: 0.199 | Val loss: 0.693 | Gen: ethay ariway onditiongway isway orkway\n",
            "Epoch:  24 | Train loss: 0.135 | Val loss: 0.693 | Gen: ethay ariway onditingngway isway orkway\n",
            "Epoch:  25 | Train loss: 0.106 | Val loss: 0.687 | Gen: ethay ariway onditingngcay isway orkway\n",
            "Epoch:  26 | Train loss: 0.095 | Val loss: 0.674 | Gen: ethay ariway onditingcongway isway orkway\n",
            "Epoch:  27 | Train loss: 0.083 | Val loss: 0.717 | Gen: ethay ariway onditiongcay isway orkwingway\n",
            "Epoch:  28 | Train loss: 0.092 | Val loss: 0.820 | Gen: ethay ariway onditingway isway orkingway\n",
            "Epoch:  29 | Train loss: 0.205 | Val loss: 0.805 | Gen: ethay arway onditingcay isway orkway\n",
            "Epoch:  30 | Train loss: 0.130 | Val loss: 0.758 | Gen: ethay ariway onditingngway isway orkwgway\n",
            "Epoch:  31 | Train loss: 0.091 | Val loss: 0.721 | Gen: ethay airay onditongcay isway ookingwray\n",
            "Epoch:  32 | Train loss: 0.077 | Val loss: 0.822 | Gen: ethay irarway onditiongway isway orkwingwray\n",
            "Epoch:  33 | Train loss: 0.068 | Val loss: 0.651 | Gen: ethay ariway onditiongconay isway orkwingwryay\n",
            "Epoch:  34 | Train loss: 0.053 | Val loss: 0.667 | Gen: ethay ariway onditiongcay isway orkingwray\n",
            "Epoch:  35 | Train loss: 0.049 | Val loss: 0.696 | Gen: ethay ariway onditingcongcay isway orkigngwray\n",
            "Epoch:  36 | Train loss: 0.059 | Val loss: 0.724 | Gen: ethay ariway onditiongcay isway orkingwray\n",
            "Epoch:  37 | Train loss: 0.082 | Val loss: 0.743 | Gen: ethay ariway ondoitongcay isway orkwiggnay\n",
            "Epoch:  38 | Train loss: 0.105 | Val loss: 0.748 | Gen: eththay ariway onditiongcay isway orkwaingway\n",
            "Epoch:  39 | Train loss: 0.117 | Val loss: 0.975 | Gen: ethay ariway ondittiongcay issway orkway\n",
            "Epoch:  40 | Train loss: 0.094 | Val loss: 0.799 | Gen: ethay aiwray onditongway isway ooxingway\n",
            "Epoch:  41 | Train loss: 0.080 | Val loss: 0.767 | Gen: ethay irarway onditiongcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.114 | Val loss: 0.628 | Gen: ethay airray onditiongcay isway orkwingwray\n",
            "Epoch:  43 | Train loss: 0.084 | Val loss: 0.665 | Gen: ethay iraway onditiongcay isway orkingwryay\n",
            "Epoch:  44 | Train loss: 0.058 | Val loss: 0.589 | Gen: ethay iraway ondotingcinay isway orkway\n",
            "Epoch:  45 | Train loss: 0.038 | Val loss: 0.645 | Gen: ethay iray onditiongcinay isway orkingwryay\n",
            "Epoch:  46 | Train loss: 0.031 | Val loss: 0.575 | Gen: ethay iray onditiongcay isway orkingwray\n",
            "Epoch:  47 | Train loss: 0.022 | Val loss: 0.586 | Gen: ethay airway onditiongcay isway orkingwray\n",
            "Epoch:  48 | Train loss: 0.022 | Val loss: 0.584 | Gen: ethay iraway onditiongcay isway orkingwray\n",
            "Epoch:  49 | Train loss: 0.016 | Val loss: 0.582 | Gen: ethay airway onditiongcay isway orkingwray\n",
            "Obtained lowest validation loss of: 0.5747334546531024\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditiongcay isway orkingwray\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "trans64_args_s = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_small\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 50,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 20,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 64,  # Increased model size\n",
        "    \"encoder_type\": \"transformer\",\n",
        "    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n",
        "    \"num_transformer_layers\": 3,\n",
        "}\n",
        "trans64_args_s.update(args_dict)\n",
        "print_opts(trans64_args_s)\n",
        "\n",
        "trans64_encoder_s, trans64_decoder_s, trans64_losses_s = train(trans64_args_s)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans64_encoder_s, trans64_decoder_s, None, trans64_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dardK4RWvUWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e71c00dc-0d5c-4be1-ca48-cc999f30df54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 20                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('ninja', 'injanay')\n",
            "('iteration', 'iterationway')\n",
            "('chick', 'ickchay')\n",
            "('innovation', 'innovationway')\n",
            "('eclat', 'eclatway')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.474 | Val loss: 2.076 | Gen: etay iway intay-intay isssay inway\n",
            "Epoch:   1 | Train loss: 1.718 | Val loss: 1.800 | Gen: etay iway ongay-intinay issssay inway\n",
            "Epoch:   2 | Train loss: 1.428 | Val loss: 1.710 | Gen: etay irway ontionintininhinhind iwayssway ongrinway\n",
            "Epoch:   3 | Train loss: 1.249 | Val loss: 1.566 | Gen: ethethetay irwayiray onginiongindindindin iwayssay orway\n",
            "Epoch:   4 | Train loss: 1.085 | Val loss: 1.465 | Gen: eway iway ontinioinginay iway owway\n",
            "Epoch:   5 | Train loss: 0.957 | Val loss: 1.505 | Gen: eway iway ontigdinay iway owway\n",
            "Epoch:   6 | Train loss: 0.890 | Val loss: 1.248 | Gen: etay iraway ontingtiongngnay isay orway\n",
            "Epoch:   7 | Train loss: 0.778 | Val loss: 1.368 | Gen: eway iway ondintay iway orway\n",
            "Epoch:   8 | Train loss: 0.720 | Val loss: 1.124 | Gen: etay iraway onditingay iway orway\n",
            "Epoch:   9 | Train loss: 0.622 | Val loss: 1.093 | Gen: etay iraway onditiongtinay-inday isay orway-ingway\n",
            "Epoch:  10 | Train loss: 0.551 | Val loss: 1.252 | Gen: eay araway onditigatinay iway orway\n",
            "Epoch:  11 | Train loss: 0.501 | Val loss: 1.163 | Gen: etay iraway onditiginayinay isay orway\n",
            "Epoch:  12 | Train loss: 0.437 | Val loss: 0.976 | Gen: etay arway onditiginay-inay isway orwingray\n",
            "Epoch:  13 | Train loss: 0.369 | Val loss: 0.931 | Gen: etay araway onditininginay isway orwingway\n",
            "Epoch:  14 | Train loss: 0.336 | Val loss: 0.957 | Gen: etay araway onditinginingcay iway orwingfay\n",
            "Epoch:  15 | Train loss: 0.317 | Val loss: 0.933 | Gen: etay araway onditintingnay iway orwingfay\n",
            "Epoch:  16 | Train loss: 0.299 | Val loss: 0.859 | Gen: etay araway onditintiniongscay isway orkingsway\n",
            "Epoch:  17 | Train loss: 0.280 | Val loss: 0.907 | Gen: ethay araway onditiniongincay isway orkingsway\n",
            "Epoch:  18 | Train loss: 0.273 | Val loss: 0.833 | Gen: ehetay arway onditintingnay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.230 | Val loss: 0.764 | Gen: eheway arway ondititingningcay isway orkinggway\n",
            "Epoch:  20 | Train loss: 0.192 | Val loss: 0.780 | Gen: ehtay arway ondititingiongcay isway orkingsway\n",
            "Epoch:  21 | Train loss: 0.173 | Val loss: 0.865 | Gen: ethay arway ondititiniogcay isway orkingsway\n",
            "Epoch:  22 | Train loss: 0.164 | Val loss: 0.796 | Gen: ethay irway ondititingincay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.186 | Val loss: 1.111 | Gen: eetay arawaywayay onditionigingcay isay orkinggway\n",
            "Epoch:  24 | Train loss: 0.329 | Val loss: 1.125 | Gen: ethay iraway onditiongioingcioway isway orkinggway\n",
            "Epoch:  25 | Train loss: 0.417 | Val loss: 0.993 | Gen: etheay ariway onditiongnay-inay isway orkingingway\n",
            "Epoch:  26 | Train loss: 0.309 | Val loss: 0.840 | Gen: ehetay ariway onnditiongnincay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.233 | Val loss: 0.769 | Gen: ethay ariway onditioningincay isway orkingggwnay\n",
            "Epoch:  28 | Train loss: 0.196 | Val loss: 0.680 | Gen: ehetay ariway onditiongincay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.149 | Val loss: 0.667 | Gen: ethay ariway onditionginincay isway orkingnway\n",
            "Epoch:  30 | Train loss: 0.123 | Val loss: 0.656 | Gen: ethay ariway onditionginay isway orkinggway\n",
            "Epoch:  31 | Train loss: 0.105 | Val loss: 0.666 | Gen: ethay ariway onditioningincay isway orkinggway\n",
            "Epoch:  32 | Train loss: 0.096 | Val loss: 0.663 | Gen: ethay ariway onditioningincay isway orkinggway\n",
            "Epoch:  33 | Train loss: 0.088 | Val loss: 0.668 | Gen: ethay airway onditioningincay isway orkinggway\n",
            "Epoch:  34 | Train loss: 0.081 | Val loss: 0.659 | Gen: ethay airway onditioningincay isway orkinggway\n",
            "Epoch:  35 | Train loss: 0.076 | Val loss: 0.663 | Gen: ethay airway onditioningincay isway orkinggway\n",
            "Epoch:  36 | Train loss: 0.071 | Val loss: 0.656 | Gen: ethay airway onditioningincay isway orkinggway\n",
            "Epoch:  37 | Train loss: 0.067 | Val loss: 0.652 | Gen: ethay airway onditioningincay isway orkinggway\n",
            "Epoch:  38 | Train loss: 0.062 | Val loss: 0.654 | Gen: ethay airway onditioningincay isway orkinggway\n",
            "Epoch:  39 | Train loss: 0.059 | Val loss: 0.651 | Gen: ethay airway onditioningincay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.055 | Val loss: 0.660 | Gen: ethay airway onditioningincay isway orkinggway\n",
            "Epoch:  41 | Train loss: 0.052 | Val loss: 0.664 | Gen: ethay airway onditioningincay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.049 | Val loss: 0.651 | Gen: ehetay airway onditioningincay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.046 | Val loss: 0.661 | Gen: ethay airway onditioningincay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.043 | Val loss: 0.723 | Gen: ehetay airway onditioningincay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.046 | Val loss: 0.681 | Gen: ethay airway onditioningincay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.067 | Val loss: 1.026 | Gen: ethay airway onditioningincay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.221 | Val loss: 1.534 | Gen: theway irrway ondiitionisingcay isway orkinggswray\n",
            "Epoch:  48 | Train loss: 0.333 | Val loss: 0.967 | Gen: eethay airway onditioningincay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.235 | Val loss: 0.667 | Gen: ethay ariway onditioningincay isway orkingway\n",
            "Obtained lowest validation loss of: 0.650605135364458\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay ariway onditioningincay isway orkingway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "trans64_args_l = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_large\",  # Increased data set size\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 50,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 20,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 512,\n",
        "    \"hidden_size\": 64,  # Increased model size\n",
        "    \"encoder_type\": \"transformer\",\n",
        "    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n",
        "    \"num_transformer_layers\": 3,\n",
        "}\n",
        "trans64_args_l.update(args_dict)\n",
        "print_opts(trans64_args_l)\n",
        "\n",
        "trans64_encoder_l, trans64_decoder_l, trans64_losses_l = train(trans64_args_l)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans64_encoder_l, trans64_decoder_l, None, trans64_args_l\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSSyiG39vVlN"
      },
      "source": [
        "The following cell generates two loss plots. In the first plot, we compare the effects of increasing dataset size. In the second plot, we compare the effects of increasing model size. Include both plots in your report, and include your analysis of the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ql0pxrEvVP6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5bebc486-22d2-4edd-f46c-f55c0bc389a0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "save_loss_comparison_by_dataset(\n",
        "    trans32_losses_s,\n",
        "    trans32_losses_l,\n",
        "    trans64_losses_s,\n",
        "    trans64_losses_l,\n",
        "    trans32_args_s,\n",
        "    trans32_args_l,\n",
        "    trans64_args_s,\n",
        "    trans64_args_l,\n",
        "    \"trans_by_dataset\",\n",
        ")\n",
        "save_loss_comparison_by_hidden(\n",
        "    trans32_losses_s,\n",
        "    trans32_losses_l,\n",
        "    trans64_losses_s,\n",
        "    trans64_losses_l,\n",
        "    trans32_args_s,\n",
        "    trans32_args_l,\n",
        "    trans64_args_s,\n",
        "    trans64_args_l,\n",
        "    \"trans_by_hidden\",\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "s9IS9B9-yUU5",
        "4BIpGwANoQOg",
        "pbvpn4MaV0I1",
        "bRWfRdmVVjUl",
        "0yh08KhgnA30"
      ],
      "name": "nmt.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}